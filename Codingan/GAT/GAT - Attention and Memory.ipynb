{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZWXxsyCOJBd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_onehot(labels):\n",
        "    # The classes must be sorted before encoding to enable static class encoding.\n",
        "    # In other words, make sure the first class always maps to index 0.\n",
        "    classes = sorted(list(set(labels)))\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
        "    return labels_onehot"
      ],
      "metadata": {
        "id": "yNWgrYPMOUQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path=\"/content/sample_data/cora\", dataset=\"cora\"):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path, dataset), dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize_features(features)\n",
        "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test\n"
      ],
      "metadata": {
        "id": "-ZO5Yd1jObbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_adj(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)"
      ],
      "metadata": {
        "id": "Ugt8ocbmOm4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_features(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ],
      "metadata": {
        "id": "L_N78_iNOqwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)"
      ],
      "metadata": {
        "id": "z1EaLQvXOukH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "S-aezBLUPG5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, h, adj):\n",
        "        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
        "        e = self._prepare_attentional_mechanism_input(Wh)\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime = torch.matmul(attention, Wh)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "    def _prepare_attentional_mechanism_input(self, Wh):\n",
        "        # Wh.shape (N, out_feature)\n",
        "        # self.a.shape (2 * out_feature, 1)\n",
        "        # Wh1&2.shape (N, 1)\n",
        "        # e.shape (N, N)\n",
        "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
        "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
        "        # broadcast add\n",
        "        e = Wh1 + Wh2.T\n",
        "        return self.leakyrelu(e)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
      ],
      "metadata": {
        "id": "79plQuRLPIZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpecialSpmmFunction(torch.autograd.Function):\n",
        "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, indices, values, shape, b):\n",
        "        assert indices.requires_grad == False\n",
        "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
        "        ctx.save_for_backward(a, b)\n",
        "        ctx.N = shape[0]\n",
        "        return torch.matmul(a, b)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        a, b = ctx.saved_tensors\n",
        "        grad_values = grad_b = None\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            grad_a_dense = grad_output.matmul(b.t())\n",
        "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
        "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
        "        if ctx.needs_input_grad[3]:\n",
        "            grad_b = a.t().matmul(grad_output)\n",
        "        return None, grad_values, None, grad_b"
      ],
      "metadata": {
        "id": "-vHH0RIBPV9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpecialSpmm(nn.Module):\n",
        "    def forward(self, indices, values, shape, b):\n",
        "        return SpecialSpmmFunction.apply(indices, values, shape, b)"
      ],
      "metadata": {
        "id": "1Ou25e4JPave"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpGraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(SpGraphAttentionLayer, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
        "                \n",
        "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
        "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "        self.special_spmm = SpecialSpmm()\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
        "\n",
        "        N = input.size()[0]\n",
        "        edge = adj.nonzero().t()\n",
        "\n",
        "        h = torch.mm(input, self.W)\n",
        "        # h: N x out\n",
        "        assert not torch.isnan(h).any()\n",
        "\n",
        "        # Self-attention on the nodes - Shared attention mechanism\n",
        "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
        "        # edge: 2*D x E\n",
        "\n",
        "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
        "        assert not torch.isnan(edge_e).any()\n",
        "        # edge_e: E\n",
        "\n",
        "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
        "        # e_rowsum: N x 1\n",
        "\n",
        "        edge_e = self.dropout(edge_e)\n",
        "        # edge_e: E\n",
        "\n",
        "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
        "        assert not torch.isnan(h_prime).any()\n",
        "        # h_prime: N x out\n",
        "        \n",
        "        h_prime = h_prime.div(e_rowsum)\n",
        "        # h_prime: N x out\n",
        "        assert not torch.isnan(h_prime).any()\n",
        "\n",
        "        if self.concat:\n",
        "            # if this layer is not last layer,\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            # if this layer is last layer,\n",
        "            return h_prime\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
      ],
      "metadata": {
        "id": "9ussoSxlPdYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "9ZYleyAGPl1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
        "        \"\"\"Dense version of GAT.\"\"\"\n",
        "        super(GAT, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.elu(self.out_att(x, adj))\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "avAPdCj8P_XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpGAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
        "        \"\"\"Sparse version of GAT.\"\"\"\n",
        "        super(SpGAT, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attentions = [SpGraphAttentionLayer(nfeat, \n",
        "                                                 nhid, \n",
        "                                                 dropout=dropout, \n",
        "                                                 alpha=alpha, \n",
        "                                                 concat=True) for _ in range(nheads)]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = SpGraphAttentionLayer(nhid * nheads, \n",
        "                                             nclass, \n",
        "                                             dropout=dropout, \n",
        "                                             alpha=alpha, \n",
        "                                             concat=False)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.elu(self.out_att(x, adj))\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "34QUjX8OQA4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "gYtxcGR5QFZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()"
      ],
      "metadata": {
        "id": "wKjSZOkQQMKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
        "parser.add_argument('--fastmode', action='store_true', default=False, help='Validate during training pass.')\n",
        "parser.add_argument('--sparse', action='store_true', default=False, help='GAT with sparse version or not.')\n",
        "parser.add_argument('--seed', type=int, default=72, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=10000, help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=0.005, help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=8, help='Number of hidden units.')\n",
        "parser.add_argument('--nb_heads', type=int, default=8, help='Number of head attentions.')\n",
        "parser.add_argument('--dropout', type=float, default=0.6, help='Dropout rate (1 - keep probability).')\n",
        "parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
        "parser.add_argument('--patience', type=int, default=100, help='Patience')\n",
        "parser.add_argument('-f')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsePRecyQPbz",
        "outputId": "8e996fa7-74bf-4ead-d607-3ca2aced1fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['-f'], dest='f', nargs=None, const=None, default=None, type=None, choices=None, help=None, metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = parser.parse_args()\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "JV98xakiZL9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)"
      ],
      "metadata": {
        "id": "nUC93cCZQTAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cry4lXZqRKJG",
        "outputId": "2e69458e-c1e7-4072-92b9-acc9974eb2ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if args.sparse:\n",
        "    model = SpGAT(nfeat=features.shape[1], \n",
        "                nhid=args.hidden, \n",
        "                nclass=int(labels.max()) + 1, \n",
        "                dropout=args.dropout, \n",
        "                nheads=args.nb_heads, \n",
        "                alpha=args.alpha)\n",
        "else:\n",
        "    model = GAT(nfeat=features.shape[1], \n",
        "                nhid=args.hidden, \n",
        "                nclass=int(labels.max()) + 1, \n",
        "                dropout=args.dropout, \n",
        "                nheads=args.nb_heads, \n",
        "                alpha=args.alpha)\n",
        "optimizer = optim.Adam(model.parameters(), \n",
        "                       lr=args.lr, \n",
        "                       weight_decay=args.weight_decay)"
      ],
      "metadata": {
        "id": "ZwbBqA37gauf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "features, adj, labels = Variable(features), Variable(adj), Variable(labels)"
      ],
      "metadata": {
        "id": "FOr1Jol2SWVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if not args.fastmode:\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_val.data.item()\n"
      ],
      "metadata": {
        "id": "YWE1JVQoSb0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.data.item()))"
      ],
      "metadata": {
        "id": "KtxL29DzSfGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = args.epochs + 1\n",
        "best_epoch = 0\n",
        "for epoch in range(args.epochs):\n",
        "    loss_values.append(train(epoch))\n",
        "\n",
        "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "    if loss_values[-1] < best:\n",
        "        best = loss_values[-1]\n",
        "        best_epoch = epoch\n",
        "        bad_counter = 0\n",
        "    else:\n",
        "        bad_counter += 1\n",
        "\n",
        "    if bad_counter == args.patience:\n",
        "        break\n",
        "\n",
        "    files = glob.glob('*.pkl')\n",
        "    for file in files:\n",
        "        epoch_nb = int(file.split('.')[0])\n",
        "        if epoch_nb < best_epoch:\n",
        "            os.remove(file)\n",
        "\n",
        "    files = glob.glob('*.pkl')\n",
        "    for file in files:\n",
        "        epoch_nb = int(file.split('.')[0])\n",
        "        if epoch_nb > best_epoch:\n",
        "            os.remove(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-vFJ5ytSi3b",
        "outputId": "4ad604b4-7d51-42b0-c2aa-04bdb5d3f0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9519 acc_train: 0.0643 loss_val: 1.9409 acc_val: 0.2833 time: 2.7795s\n",
            "Epoch: 0002 loss_train: 1.9377 acc_train: 0.1714 loss_val: 1.9315 acc_val: 0.4933 time: 0.0932s\n",
            "Epoch: 0003 loss_train: 1.9303 acc_train: 0.2643 loss_val: 1.9219 acc_val: 0.5300 time: 0.0853s\n",
            "Epoch: 0004 loss_train: 1.9168 acc_train: 0.3929 loss_val: 1.9123 acc_val: 0.5433 time: 0.0741s\n",
            "Epoch: 0005 loss_train: 1.9068 acc_train: 0.4786 loss_val: 1.9026 acc_val: 0.5433 time: 0.0737s\n",
            "Epoch: 0006 loss_train: 1.8953 acc_train: 0.5071 loss_val: 1.8930 acc_val: 0.5433 time: 0.0733s\n",
            "Epoch: 0007 loss_train: 1.8806 acc_train: 0.5214 loss_val: 1.8833 acc_val: 0.5467 time: 0.0733s\n",
            "Epoch: 0008 loss_train: 1.8675 acc_train: 0.4929 loss_val: 1.8735 acc_val: 0.5467 time: 0.0733s\n",
            "Epoch: 0009 loss_train: 1.8548 acc_train: 0.5000 loss_val: 1.8634 acc_val: 0.5500 time: 0.0734s\n",
            "Epoch: 0010 loss_train: 1.8406 acc_train: 0.5357 loss_val: 1.8532 acc_val: 0.5433 time: 0.0733s\n",
            "Epoch: 0011 loss_train: 1.8482 acc_train: 0.5429 loss_val: 1.8429 acc_val: 0.5400 time: 0.0733s\n",
            "Epoch: 0012 loss_train: 1.8183 acc_train: 0.4643 loss_val: 1.8323 acc_val: 0.5367 time: 0.0733s\n",
            "Epoch: 0013 loss_train: 1.8019 acc_train: 0.5143 loss_val: 1.8216 acc_val: 0.5433 time: 0.0733s\n",
            "Epoch: 0014 loss_train: 1.7961 acc_train: 0.5357 loss_val: 1.8106 acc_val: 0.5467 time: 0.0732s\n",
            "Epoch: 0015 loss_train: 1.7625 acc_train: 0.4786 loss_val: 1.7995 acc_val: 0.5500 time: 0.0732s\n",
            "Epoch: 0016 loss_train: 1.7795 acc_train: 0.4714 loss_val: 1.7883 acc_val: 0.5533 time: 0.0733s\n",
            "Epoch: 0017 loss_train: 1.7557 acc_train: 0.5143 loss_val: 1.7768 acc_val: 0.5600 time: 0.0733s\n",
            "Epoch: 0018 loss_train: 1.7333 acc_train: 0.5357 loss_val: 1.7651 acc_val: 0.5600 time: 0.0733s\n",
            "Epoch: 0019 loss_train: 1.7123 acc_train: 0.5714 loss_val: 1.7533 acc_val: 0.5633 time: 0.0733s\n",
            "Epoch: 0020 loss_train: 1.7148 acc_train: 0.4714 loss_val: 1.7415 acc_val: 0.5667 time: 0.0733s\n",
            "Epoch: 0021 loss_train: 1.6810 acc_train: 0.5786 loss_val: 1.7296 acc_val: 0.5700 time: 0.0734s\n",
            "Epoch: 0022 loss_train: 1.6757 acc_train: 0.5571 loss_val: 1.7174 acc_val: 0.5767 time: 0.0733s\n",
            "Epoch: 0023 loss_train: 1.7078 acc_train: 0.4714 loss_val: 1.7052 acc_val: 0.5767 time: 0.0734s\n",
            "Epoch: 0024 loss_train: 1.6715 acc_train: 0.5643 loss_val: 1.6930 acc_val: 0.5800 time: 0.0733s\n",
            "Epoch: 0025 loss_train: 1.6649 acc_train: 0.5857 loss_val: 1.6808 acc_val: 0.5800 time: 0.0733s\n",
            "Epoch: 0026 loss_train: 1.6399 acc_train: 0.4857 loss_val: 1.6684 acc_val: 0.5800 time: 0.0732s\n",
            "Epoch: 0027 loss_train: 1.6255 acc_train: 0.5000 loss_val: 1.6562 acc_val: 0.5800 time: 0.0733s\n",
            "Epoch: 0028 loss_train: 1.5924 acc_train: 0.5786 loss_val: 1.6439 acc_val: 0.5900 time: 0.0733s\n",
            "Epoch: 0029 loss_train: 1.6013 acc_train: 0.5500 loss_val: 1.6317 acc_val: 0.5900 time: 0.0733s\n",
            "Epoch: 0030 loss_train: 1.5530 acc_train: 0.5929 loss_val: 1.6193 acc_val: 0.5967 time: 0.0733s\n",
            "Epoch: 0031 loss_train: 1.5769 acc_train: 0.5571 loss_val: 1.6070 acc_val: 0.6000 time: 0.0733s\n",
            "Epoch: 0032 loss_train: 1.5301 acc_train: 0.6071 loss_val: 1.5946 acc_val: 0.6067 time: 0.0733s\n",
            "Epoch: 0033 loss_train: 1.5405 acc_train: 0.5500 loss_val: 1.5823 acc_val: 0.6100 time: 0.0733s\n",
            "Epoch: 0034 loss_train: 1.5684 acc_train: 0.5429 loss_val: 1.5701 acc_val: 0.6100 time: 0.0734s\n",
            "Epoch: 0035 loss_train: 1.5482 acc_train: 0.5500 loss_val: 1.5580 acc_val: 0.6167 time: 0.0733s\n",
            "Epoch: 0036 loss_train: 1.5182 acc_train: 0.6000 loss_val: 1.5460 acc_val: 0.6300 time: 0.0733s\n",
            "Epoch: 0037 loss_train: 1.4665 acc_train: 0.6000 loss_val: 1.5340 acc_val: 0.6367 time: 0.0733s\n",
            "Epoch: 0038 loss_train: 1.5028 acc_train: 0.5786 loss_val: 1.5220 acc_val: 0.6433 time: 0.0732s\n",
            "Epoch: 0039 loss_train: 1.4628 acc_train: 0.6143 loss_val: 1.5101 acc_val: 0.6433 time: 0.0733s\n",
            "Epoch: 0040 loss_train: 1.4882 acc_train: 0.5714 loss_val: 1.4983 acc_val: 0.6467 time: 0.0733s\n",
            "Epoch: 0041 loss_train: 1.4294 acc_train: 0.5929 loss_val: 1.4866 acc_val: 0.6500 time: 0.0733s\n",
            "Epoch: 0042 loss_train: 1.3883 acc_train: 0.6071 loss_val: 1.4749 acc_val: 0.6533 time: 0.0733s\n",
            "Epoch: 0043 loss_train: 1.3636 acc_train: 0.6571 loss_val: 1.4632 acc_val: 0.6533 time: 0.0734s\n",
            "Epoch: 0044 loss_train: 1.4037 acc_train: 0.5929 loss_val: 1.4516 acc_val: 0.6567 time: 0.0733s\n",
            "Epoch: 0045 loss_train: 1.3823 acc_train: 0.6500 loss_val: 1.4400 acc_val: 0.6667 time: 0.0733s\n",
            "Epoch: 0046 loss_train: 1.3347 acc_train: 0.7000 loss_val: 1.4284 acc_val: 0.6733 time: 0.0732s\n",
            "Epoch: 0047 loss_train: 1.3081 acc_train: 0.7143 loss_val: 1.4167 acc_val: 0.6800 time: 0.0733s\n",
            "Epoch: 0048 loss_train: 1.3015 acc_train: 0.6571 loss_val: 1.4051 acc_val: 0.6900 time: 0.0733s\n",
            "Epoch: 0049 loss_train: 1.3075 acc_train: 0.6786 loss_val: 1.3936 acc_val: 0.7033 time: 0.0733s\n",
            "Epoch: 0050 loss_train: 1.2855 acc_train: 0.7214 loss_val: 1.3820 acc_val: 0.7100 time: 0.0734s\n",
            "Epoch: 0051 loss_train: 1.2779 acc_train: 0.6786 loss_val: 1.3703 acc_val: 0.7167 time: 0.0732s\n",
            "Epoch: 0052 loss_train: 1.2935 acc_train: 0.7000 loss_val: 1.3589 acc_val: 0.7233 time: 0.0733s\n",
            "Epoch: 0053 loss_train: 1.2757 acc_train: 0.6714 loss_val: 1.3476 acc_val: 0.7267 time: 0.0733s\n",
            "Epoch: 0054 loss_train: 1.2819 acc_train: 0.6429 loss_val: 1.3367 acc_val: 0.7267 time: 0.0733s\n",
            "Epoch: 0055 loss_train: 1.2950 acc_train: 0.6857 loss_val: 1.3262 acc_val: 0.7333 time: 0.0732s\n",
            "Epoch: 0056 loss_train: 1.3148 acc_train: 0.6714 loss_val: 1.3159 acc_val: 0.7400 time: 0.0732s\n",
            "Epoch: 0057 loss_train: 1.2787 acc_train: 0.6786 loss_val: 1.3058 acc_val: 0.7467 time: 0.0733s\n",
            "Epoch: 0058 loss_train: 1.2849 acc_train: 0.6786 loss_val: 1.2959 acc_val: 0.7567 time: 0.0733s\n",
            "Epoch: 0059 loss_train: 1.1996 acc_train: 0.7857 loss_val: 1.2859 acc_val: 0.7600 time: 0.0734s\n",
            "Epoch: 0060 loss_train: 1.1955 acc_train: 0.7143 loss_val: 1.2760 acc_val: 0.7667 time: 0.0733s\n",
            "Epoch: 0061 loss_train: 1.2380 acc_train: 0.6857 loss_val: 1.2661 acc_val: 0.7767 time: 0.0734s\n",
            "Epoch: 0062 loss_train: 1.1973 acc_train: 0.7429 loss_val: 1.2565 acc_val: 0.7800 time: 0.0734s\n",
            "Epoch: 0063 loss_train: 1.1875 acc_train: 0.6929 loss_val: 1.2471 acc_val: 0.7833 time: 0.0734s\n",
            "Epoch: 0064 loss_train: 1.1562 acc_train: 0.7286 loss_val: 1.2377 acc_val: 0.7933 time: 0.0734s\n",
            "Epoch: 0065 loss_train: 1.2481 acc_train: 0.6929 loss_val: 1.2285 acc_val: 0.7933 time: 0.0733s\n",
            "Epoch: 0066 loss_train: 1.2367 acc_train: 0.6571 loss_val: 1.2195 acc_val: 0.7967 time: 0.0734s\n",
            "Epoch: 0067 loss_train: 1.0938 acc_train: 0.7786 loss_val: 1.2107 acc_val: 0.8033 time: 0.0733s\n",
            "Epoch: 0068 loss_train: 1.1408 acc_train: 0.7286 loss_val: 1.2019 acc_val: 0.8033 time: 0.0733s\n",
            "Epoch: 0069 loss_train: 1.0718 acc_train: 0.7357 loss_val: 1.1932 acc_val: 0.8033 time: 0.0734s\n",
            "Epoch: 0070 loss_train: 1.1561 acc_train: 0.7214 loss_val: 1.1847 acc_val: 0.8133 time: 0.0737s\n",
            "Epoch: 0071 loss_train: 1.1816 acc_train: 0.7429 loss_val: 1.1763 acc_val: 0.8167 time: 0.0733s\n",
            "Epoch: 0072 loss_train: 1.0936 acc_train: 0.7571 loss_val: 1.1679 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0073 loss_train: 1.2060 acc_train: 0.7429 loss_val: 1.1603 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0074 loss_train: 1.1196 acc_train: 0.7214 loss_val: 1.1528 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0075 loss_train: 1.1911 acc_train: 0.7214 loss_val: 1.1457 acc_val: 0.8233 time: 0.0732s\n",
            "Epoch: 0076 loss_train: 1.0480 acc_train: 0.7714 loss_val: 1.1388 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0077 loss_train: 1.1219 acc_train: 0.7357 loss_val: 1.1325 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0078 loss_train: 0.9772 acc_train: 0.8286 loss_val: 1.1261 acc_val: 0.8200 time: 0.0732s\n",
            "Epoch: 0079 loss_train: 1.0322 acc_train: 0.7000 loss_val: 1.1198 acc_val: 0.8167 time: 0.0733s\n",
            "Epoch: 0080 loss_train: 1.0684 acc_train: 0.7286 loss_val: 1.1134 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0081 loss_train: 1.0874 acc_train: 0.7500 loss_val: 1.1072 acc_val: 0.8133 time: 0.0733s\n",
            "Epoch: 0082 loss_train: 1.0006 acc_train: 0.8357 loss_val: 1.1010 acc_val: 0.8167 time: 0.0733s\n",
            "Epoch: 0083 loss_train: 0.9911 acc_train: 0.7643 loss_val: 1.0949 acc_val: 0.8167 time: 0.0733s\n",
            "Epoch: 0084 loss_train: 1.0261 acc_train: 0.7714 loss_val: 1.0892 acc_val: 0.8167 time: 0.0736s\n",
            "Epoch: 0085 loss_train: 1.0374 acc_train: 0.7786 loss_val: 1.0836 acc_val: 0.8167 time: 0.0733s\n",
            "Epoch: 0086 loss_train: 1.0364 acc_train: 0.7714 loss_val: 1.0785 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0087 loss_train: 0.9236 acc_train: 0.8071 loss_val: 1.0733 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0088 loss_train: 1.0308 acc_train: 0.7429 loss_val: 1.0681 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0089 loss_train: 0.9808 acc_train: 0.7786 loss_val: 1.0629 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0090 loss_train: 0.9552 acc_train: 0.8143 loss_val: 1.0580 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0091 loss_train: 1.0411 acc_train: 0.7500 loss_val: 1.0532 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0092 loss_train: 0.8349 acc_train: 0.8357 loss_val: 1.0481 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0093 loss_train: 1.0392 acc_train: 0.7571 loss_val: 1.0429 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0094 loss_train: 1.0159 acc_train: 0.7929 loss_val: 1.0380 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0095 loss_train: 0.9552 acc_train: 0.7571 loss_val: 1.0331 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0096 loss_train: 1.0567 acc_train: 0.7214 loss_val: 1.0285 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0097 loss_train: 0.9461 acc_train: 0.8143 loss_val: 1.0238 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0098 loss_train: 1.0001 acc_train: 0.7714 loss_val: 1.0189 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0099 loss_train: 0.9051 acc_train: 0.8071 loss_val: 1.0137 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0100 loss_train: 0.9725 acc_train: 0.7571 loss_val: 1.0083 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0101 loss_train: 1.0071 acc_train: 0.7429 loss_val: 1.0031 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0102 loss_train: 0.9707 acc_train: 0.7857 loss_val: 0.9982 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0103 loss_train: 1.0440 acc_train: 0.7429 loss_val: 0.9933 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0104 loss_train: 0.9415 acc_train: 0.8071 loss_val: 0.9884 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0105 loss_train: 0.9431 acc_train: 0.7643 loss_val: 0.9836 acc_val: 0.8233 time: 0.0732s\n",
            "Epoch: 0106 loss_train: 0.9688 acc_train: 0.7357 loss_val: 0.9787 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0107 loss_train: 0.9136 acc_train: 0.7500 loss_val: 0.9738 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0108 loss_train: 0.9057 acc_train: 0.7643 loss_val: 0.9690 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0109 loss_train: 0.9375 acc_train: 0.7786 loss_val: 0.9641 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0110 loss_train: 0.9726 acc_train: 0.7357 loss_val: 0.9594 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0111 loss_train: 0.8846 acc_train: 0.7714 loss_val: 0.9550 acc_val: 0.8333 time: 0.0735s\n",
            "Epoch: 0112 loss_train: 0.9325 acc_train: 0.7786 loss_val: 0.9506 acc_val: 0.8333 time: 0.0733s\n",
            "Epoch: 0113 loss_train: 0.9084 acc_train: 0.7429 loss_val: 0.9466 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0114 loss_train: 0.8865 acc_train: 0.8071 loss_val: 0.9429 acc_val: 0.8333 time: 0.0733s\n",
            "Epoch: 0115 loss_train: 0.9154 acc_train: 0.8071 loss_val: 0.9393 acc_val: 0.8333 time: 0.0733s\n",
            "Epoch: 0116 loss_train: 0.9027 acc_train: 0.7929 loss_val: 0.9357 acc_val: 0.8333 time: 0.0733s\n",
            "Epoch: 0117 loss_train: 0.8483 acc_train: 0.8286 loss_val: 0.9325 acc_val: 0.8333 time: 0.0733s\n",
            "Epoch: 0118 loss_train: 0.9796 acc_train: 0.7429 loss_val: 0.9291 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0119 loss_train: 0.9399 acc_train: 0.7643 loss_val: 0.9260 acc_val: 0.8333 time: 0.0733s\n",
            "Epoch: 0120 loss_train: 0.9506 acc_train: 0.7643 loss_val: 0.9229 acc_val: 0.8333 time: 0.0733s\n",
            "Epoch: 0121 loss_train: 0.8636 acc_train: 0.8000 loss_val: 0.9199 acc_val: 0.8333 time: 0.0733s\n",
            "Epoch: 0122 loss_train: 0.8189 acc_train: 0.8429 loss_val: 0.9169 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0123 loss_train: 0.9167 acc_train: 0.7571 loss_val: 0.9140 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0124 loss_train: 0.8330 acc_train: 0.8071 loss_val: 0.9112 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0125 loss_train: 0.8261 acc_train: 0.8214 loss_val: 0.9086 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0126 loss_train: 0.9700 acc_train: 0.7429 loss_val: 0.9061 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0127 loss_train: 0.8535 acc_train: 0.8286 loss_val: 0.9039 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0128 loss_train: 0.8972 acc_train: 0.7643 loss_val: 0.9016 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0129 loss_train: 0.7792 acc_train: 0.8429 loss_val: 0.8991 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0130 loss_train: 0.8138 acc_train: 0.8286 loss_val: 0.8967 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0131 loss_train: 0.8773 acc_train: 0.7357 loss_val: 0.8942 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0132 loss_train: 0.8238 acc_train: 0.7857 loss_val: 0.8915 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0133 loss_train: 0.9441 acc_train: 0.7857 loss_val: 0.8886 acc_val: 0.8267 time: 0.0736s\n",
            "Epoch: 0134 loss_train: 0.8852 acc_train: 0.7357 loss_val: 0.8858 acc_val: 0.8267 time: 0.0736s\n",
            "Epoch: 0135 loss_train: 0.8281 acc_train: 0.8071 loss_val: 0.8830 acc_val: 0.8267 time: 0.0732s\n",
            "Epoch: 0136 loss_train: 0.8102 acc_train: 0.7857 loss_val: 0.8800 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0137 loss_train: 0.8010 acc_train: 0.7643 loss_val: 0.8772 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0138 loss_train: 0.8722 acc_train: 0.8000 loss_val: 0.8746 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0139 loss_train: 0.9445 acc_train: 0.7143 loss_val: 0.8722 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0140 loss_train: 0.8038 acc_train: 0.7786 loss_val: 0.8700 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0141 loss_train: 0.8393 acc_train: 0.7786 loss_val: 0.8677 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0142 loss_train: 0.8614 acc_train: 0.7786 loss_val: 0.8654 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0143 loss_train: 0.8505 acc_train: 0.7643 loss_val: 0.8632 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0144 loss_train: 0.7332 acc_train: 0.8286 loss_val: 0.8610 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0145 loss_train: 0.8124 acc_train: 0.7786 loss_val: 0.8587 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0146 loss_train: 0.8615 acc_train: 0.8000 loss_val: 0.8566 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0147 loss_train: 0.8071 acc_train: 0.8143 loss_val: 0.8547 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0148 loss_train: 0.8011 acc_train: 0.7929 loss_val: 0.8529 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0149 loss_train: 0.8804 acc_train: 0.7786 loss_val: 0.8511 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0150 loss_train: 0.7928 acc_train: 0.7929 loss_val: 0.8494 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0151 loss_train: 0.7543 acc_train: 0.7929 loss_val: 0.8477 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0152 loss_train: 0.7435 acc_train: 0.8000 loss_val: 0.8460 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0153 loss_train: 0.7870 acc_train: 0.8429 loss_val: 0.8441 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0154 loss_train: 0.7837 acc_train: 0.8000 loss_val: 0.8422 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0155 loss_train: 0.8389 acc_train: 0.7714 loss_val: 0.8404 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0156 loss_train: 0.7381 acc_train: 0.7929 loss_val: 0.8385 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0157 loss_train: 0.8339 acc_train: 0.7929 loss_val: 0.8368 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0158 loss_train: 0.8553 acc_train: 0.7500 loss_val: 0.8352 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0159 loss_train: 0.7952 acc_train: 0.7571 loss_val: 0.8337 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0160 loss_train: 0.8159 acc_train: 0.8000 loss_val: 0.8320 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0161 loss_train: 0.8460 acc_train: 0.7714 loss_val: 0.8304 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0162 loss_train: 0.8379 acc_train: 0.7786 loss_val: 0.8288 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0163 loss_train: 0.7765 acc_train: 0.7786 loss_val: 0.8272 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0164 loss_train: 0.7505 acc_train: 0.8143 loss_val: 0.8256 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0165 loss_train: 0.7749 acc_train: 0.7929 loss_val: 0.8239 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0166 loss_train: 0.7915 acc_train: 0.7786 loss_val: 0.8222 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0167 loss_train: 0.8483 acc_train: 0.7786 loss_val: 0.8203 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0168 loss_train: 0.7138 acc_train: 0.8071 loss_val: 0.8181 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0169 loss_train: 0.7751 acc_train: 0.7929 loss_val: 0.8161 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0170 loss_train: 0.7581 acc_train: 0.8071 loss_val: 0.8143 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0171 loss_train: 0.7374 acc_train: 0.8214 loss_val: 0.8125 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0172 loss_train: 0.7518 acc_train: 0.8000 loss_val: 0.8107 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0173 loss_train: 0.8515 acc_train: 0.7571 loss_val: 0.8093 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0174 loss_train: 0.7696 acc_train: 0.8000 loss_val: 0.8078 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0175 loss_train: 0.8225 acc_train: 0.7929 loss_val: 0.8064 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0176 loss_train: 0.6596 acc_train: 0.8571 loss_val: 0.8047 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0177 loss_train: 0.7789 acc_train: 0.7857 loss_val: 0.8032 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0178 loss_train: 0.7808 acc_train: 0.8000 loss_val: 0.8016 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0179 loss_train: 0.9029 acc_train: 0.7429 loss_val: 0.8002 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0180 loss_train: 0.7495 acc_train: 0.7929 loss_val: 0.7988 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0181 loss_train: 0.7311 acc_train: 0.7929 loss_val: 0.7972 acc_val: 0.8333 time: 0.0733s\n",
            "Epoch: 0182 loss_train: 0.7035 acc_train: 0.8286 loss_val: 0.7955 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0183 loss_train: 0.6841 acc_train: 0.8429 loss_val: 0.7938 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0184 loss_train: 0.7316 acc_train: 0.7857 loss_val: 0.7919 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0185 loss_train: 0.7405 acc_train: 0.8071 loss_val: 0.7898 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0186 loss_train: 0.7626 acc_train: 0.8214 loss_val: 0.7878 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0187 loss_train: 0.7211 acc_train: 0.7786 loss_val: 0.7858 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0188 loss_train: 0.6780 acc_train: 0.8429 loss_val: 0.7841 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0189 loss_train: 0.7091 acc_train: 0.8071 loss_val: 0.7823 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0190 loss_train: 0.7833 acc_train: 0.8214 loss_val: 0.7805 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0191 loss_train: 0.8626 acc_train: 0.7214 loss_val: 0.7790 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0192 loss_train: 0.7647 acc_train: 0.7786 loss_val: 0.7775 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0193 loss_train: 0.6879 acc_train: 0.8357 loss_val: 0.7760 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0194 loss_train: 0.8221 acc_train: 0.7500 loss_val: 0.7745 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0195 loss_train: 0.7273 acc_train: 0.7857 loss_val: 0.7729 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0196 loss_train: 0.7483 acc_train: 0.8214 loss_val: 0.7714 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0197 loss_train: 0.7029 acc_train: 0.8429 loss_val: 0.7699 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0198 loss_train: 0.7296 acc_train: 0.7929 loss_val: 0.7683 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0199 loss_train: 0.8176 acc_train: 0.7786 loss_val: 0.7670 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0200 loss_train: 0.7303 acc_train: 0.8429 loss_val: 0.7657 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0201 loss_train: 0.7161 acc_train: 0.8286 loss_val: 0.7644 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0202 loss_train: 0.7076 acc_train: 0.8429 loss_val: 0.7630 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0203 loss_train: 0.7882 acc_train: 0.7500 loss_val: 0.7617 acc_val: 0.8233 time: 0.0736s\n",
            "Epoch: 0204 loss_train: 0.7324 acc_train: 0.8000 loss_val: 0.7607 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0205 loss_train: 0.6836 acc_train: 0.8357 loss_val: 0.7597 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0206 loss_train: 0.6395 acc_train: 0.7929 loss_val: 0.7589 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0207 loss_train: 0.7236 acc_train: 0.8214 loss_val: 0.7581 acc_val: 0.8167 time: 0.0733s\n",
            "Epoch: 0208 loss_train: 0.6914 acc_train: 0.8214 loss_val: 0.7574 acc_val: 0.8200 time: 0.0736s\n",
            "Epoch: 0209 loss_train: 0.7570 acc_train: 0.7857 loss_val: 0.7569 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0210 loss_train: 0.7116 acc_train: 0.8143 loss_val: 0.7561 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0211 loss_train: 0.7826 acc_train: 0.8071 loss_val: 0.7556 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0212 loss_train: 0.7833 acc_train: 0.7857 loss_val: 0.7551 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0213 loss_train: 0.6844 acc_train: 0.8500 loss_val: 0.7545 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0214 loss_train: 0.7407 acc_train: 0.8214 loss_val: 0.7541 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0215 loss_train: 0.7219 acc_train: 0.7857 loss_val: 0.7535 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0216 loss_train: 0.7356 acc_train: 0.8000 loss_val: 0.7529 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0217 loss_train: 0.6907 acc_train: 0.8214 loss_val: 0.7523 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0218 loss_train: 0.7956 acc_train: 0.7571 loss_val: 0.7518 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0219 loss_train: 0.6114 acc_train: 0.8643 loss_val: 0.7513 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0220 loss_train: 0.8096 acc_train: 0.7857 loss_val: 0.7509 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0221 loss_train: 0.7278 acc_train: 0.8071 loss_val: 0.7506 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0222 loss_train: 0.7775 acc_train: 0.7714 loss_val: 0.7498 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0223 loss_train: 0.7914 acc_train: 0.7929 loss_val: 0.7492 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0224 loss_train: 0.7030 acc_train: 0.8500 loss_val: 0.7486 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0225 loss_train: 0.7832 acc_train: 0.7500 loss_val: 0.7478 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0226 loss_train: 0.7081 acc_train: 0.8000 loss_val: 0.7471 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0227 loss_train: 0.7644 acc_train: 0.8143 loss_val: 0.7464 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0228 loss_train: 0.8021 acc_train: 0.7714 loss_val: 0.7456 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0229 loss_train: 0.7952 acc_train: 0.7571 loss_val: 0.7448 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0230 loss_train: 0.7235 acc_train: 0.8071 loss_val: 0.7440 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0231 loss_train: 0.7398 acc_train: 0.8000 loss_val: 0.7434 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0232 loss_train: 0.6805 acc_train: 0.8000 loss_val: 0.7427 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0233 loss_train: 0.7665 acc_train: 0.8286 loss_val: 0.7419 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0234 loss_train: 0.7940 acc_train: 0.7286 loss_val: 0.7409 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0235 loss_train: 0.6574 acc_train: 0.8286 loss_val: 0.7399 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0236 loss_train: 0.5960 acc_train: 0.8429 loss_val: 0.7390 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0237 loss_train: 0.6715 acc_train: 0.8214 loss_val: 0.7382 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0238 loss_train: 0.7646 acc_train: 0.8143 loss_val: 0.7372 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0239 loss_train: 0.6918 acc_train: 0.8143 loss_val: 0.7364 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0240 loss_train: 0.8904 acc_train: 0.6929 loss_val: 0.7359 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0241 loss_train: 0.7895 acc_train: 0.7857 loss_val: 0.7357 acc_val: 0.8300 time: 0.0738s\n",
            "Epoch: 0242 loss_train: 0.7917 acc_train: 0.7643 loss_val: 0.7358 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0243 loss_train: 0.6935 acc_train: 0.8000 loss_val: 0.7357 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0244 loss_train: 0.6834 acc_train: 0.8214 loss_val: 0.7353 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0245 loss_train: 0.6675 acc_train: 0.8500 loss_val: 0.7347 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0246 loss_train: 0.5980 acc_train: 0.8643 loss_val: 0.7342 acc_val: 0.8167 time: 0.0733s\n",
            "Epoch: 0247 loss_train: 0.7725 acc_train: 0.8000 loss_val: 0.7333 acc_val: 0.8167 time: 0.0733s\n",
            "Epoch: 0248 loss_train: 0.7081 acc_train: 0.8214 loss_val: 0.7324 acc_val: 0.8167 time: 0.0733s\n",
            "Epoch: 0249 loss_train: 0.7067 acc_train: 0.8357 loss_val: 0.7314 acc_val: 0.8167 time: 0.0733s\n",
            "Epoch: 0250 loss_train: 0.6762 acc_train: 0.8500 loss_val: 0.7302 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0251 loss_train: 0.8100 acc_train: 0.7357 loss_val: 0.7291 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0252 loss_train: 0.6363 acc_train: 0.8000 loss_val: 0.7280 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0253 loss_train: 0.6503 acc_train: 0.8500 loss_val: 0.7268 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0254 loss_train: 0.6531 acc_train: 0.8286 loss_val: 0.7256 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0255 loss_train: 0.7346 acc_train: 0.8000 loss_val: 0.7243 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0256 loss_train: 0.7098 acc_train: 0.7929 loss_val: 0.7230 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0257 loss_train: 0.6831 acc_train: 0.8357 loss_val: 0.7219 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0258 loss_train: 0.6988 acc_train: 0.7857 loss_val: 0.7208 acc_val: 0.8233 time: 0.0740s\n",
            "Epoch: 0259 loss_train: 0.7063 acc_train: 0.8286 loss_val: 0.7199 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0260 loss_train: 0.8138 acc_train: 0.7714 loss_val: 0.7194 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0261 loss_train: 0.7008 acc_train: 0.8214 loss_val: 0.7189 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0262 loss_train: 0.7032 acc_train: 0.8071 loss_val: 0.7184 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0263 loss_train: 0.5987 acc_train: 0.8429 loss_val: 0.7179 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0264 loss_train: 0.7267 acc_train: 0.7929 loss_val: 0.7175 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0265 loss_train: 0.6333 acc_train: 0.8357 loss_val: 0.7169 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0266 loss_train: 0.7114 acc_train: 0.7929 loss_val: 0.7163 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0267 loss_train: 0.7206 acc_train: 0.7571 loss_val: 0.7159 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0268 loss_train: 0.6705 acc_train: 0.8071 loss_val: 0.7156 acc_val: 0.8233 time: 0.0736s\n",
            "Epoch: 0269 loss_train: 0.7034 acc_train: 0.8357 loss_val: 0.7153 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0270 loss_train: 0.6391 acc_train: 0.8500 loss_val: 0.7153 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0271 loss_train: 0.7265 acc_train: 0.7643 loss_val: 0.7151 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0272 loss_train: 0.6879 acc_train: 0.8286 loss_val: 0.7148 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0273 loss_train: 0.5786 acc_train: 0.8714 loss_val: 0.7142 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0274 loss_train: 0.6626 acc_train: 0.7929 loss_val: 0.7138 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0275 loss_train: 0.7460 acc_train: 0.7929 loss_val: 0.7131 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0276 loss_train: 0.6759 acc_train: 0.8143 loss_val: 0.7125 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0277 loss_train: 0.7393 acc_train: 0.7857 loss_val: 0.7117 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0278 loss_train: 0.6334 acc_train: 0.8571 loss_val: 0.7107 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0279 loss_train: 0.6218 acc_train: 0.8214 loss_val: 0.7098 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0280 loss_train: 0.6845 acc_train: 0.7786 loss_val: 0.7090 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0281 loss_train: 0.6866 acc_train: 0.8429 loss_val: 0.7081 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0282 loss_train: 0.7587 acc_train: 0.7929 loss_val: 0.7072 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0283 loss_train: 0.6993 acc_train: 0.8071 loss_val: 0.7067 acc_val: 0.8200 time: 0.0736s\n",
            "Epoch: 0284 loss_train: 0.6880 acc_train: 0.8143 loss_val: 0.7061 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0285 loss_train: 0.7460 acc_train: 0.7857 loss_val: 0.7055 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0286 loss_train: 0.6504 acc_train: 0.8071 loss_val: 0.7051 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0287 loss_train: 0.6487 acc_train: 0.8143 loss_val: 0.7045 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0288 loss_train: 0.7043 acc_train: 0.7929 loss_val: 0.7038 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0289 loss_train: 0.7032 acc_train: 0.7643 loss_val: 0.7033 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0290 loss_train: 0.6462 acc_train: 0.8500 loss_val: 0.7029 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0291 loss_train: 0.6389 acc_train: 0.8571 loss_val: 0.7027 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0292 loss_train: 0.6588 acc_train: 0.8571 loss_val: 0.7025 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0293 loss_train: 0.6843 acc_train: 0.8000 loss_val: 0.7022 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0294 loss_train: 0.7564 acc_train: 0.7857 loss_val: 0.7020 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0295 loss_train: 0.6041 acc_train: 0.8214 loss_val: 0.7020 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0296 loss_train: 0.6890 acc_train: 0.7857 loss_val: 0.7019 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0297 loss_train: 0.6389 acc_train: 0.7929 loss_val: 0.7022 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0298 loss_train: 0.6611 acc_train: 0.8286 loss_val: 0.7026 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0299 loss_train: 0.7816 acc_train: 0.7571 loss_val: 0.7029 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0300 loss_train: 0.7237 acc_train: 0.7857 loss_val: 0.7034 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0301 loss_train: 0.6045 acc_train: 0.8500 loss_val: 0.7037 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0302 loss_train: 0.6776 acc_train: 0.8000 loss_val: 0.7037 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0303 loss_train: 0.6610 acc_train: 0.7929 loss_val: 0.7038 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0304 loss_train: 0.6480 acc_train: 0.8429 loss_val: 0.7035 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0305 loss_train: 0.6779 acc_train: 0.7857 loss_val: 0.7033 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0306 loss_train: 0.7427 acc_train: 0.7714 loss_val: 0.7032 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0307 loss_train: 0.6019 acc_train: 0.8500 loss_val: 0.7030 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0308 loss_train: 0.6815 acc_train: 0.8571 loss_val: 0.7025 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0309 loss_train: 0.7106 acc_train: 0.7929 loss_val: 0.7017 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0310 loss_train: 0.6324 acc_train: 0.8357 loss_val: 0.7011 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0311 loss_train: 0.6766 acc_train: 0.8071 loss_val: 0.7005 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0312 loss_train: 0.6552 acc_train: 0.8571 loss_val: 0.6995 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0313 loss_train: 0.6648 acc_train: 0.8357 loss_val: 0.6986 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0314 loss_train: 0.6715 acc_train: 0.8429 loss_val: 0.6979 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0315 loss_train: 0.5930 acc_train: 0.8500 loss_val: 0.6971 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0316 loss_train: 0.5611 acc_train: 0.8571 loss_val: 0.6963 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0317 loss_train: 0.7058 acc_train: 0.8214 loss_val: 0.6955 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0318 loss_train: 0.7392 acc_train: 0.7714 loss_val: 0.6949 acc_val: 0.8333 time: 0.0735s\n",
            "Epoch: 0319 loss_train: 0.6108 acc_train: 0.8357 loss_val: 0.6944 acc_val: 0.8333 time: 0.0735s\n",
            "Epoch: 0320 loss_train: 0.7281 acc_train: 0.8000 loss_val: 0.6943 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0321 loss_train: 0.6633 acc_train: 0.8143 loss_val: 0.6940 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0322 loss_train: 0.6059 acc_train: 0.8429 loss_val: 0.6935 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0323 loss_train: 0.8030 acc_train: 0.7786 loss_val: 0.6926 acc_val: 0.8367 time: 0.0733s\n",
            "Epoch: 0324 loss_train: 0.6767 acc_train: 0.8143 loss_val: 0.6916 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0325 loss_train: 0.7213 acc_train: 0.7929 loss_val: 0.6906 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0326 loss_train: 0.6799 acc_train: 0.8000 loss_val: 0.6898 acc_val: 0.8367 time: 0.0733s\n",
            "Epoch: 0327 loss_train: 0.6187 acc_train: 0.8571 loss_val: 0.6890 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0328 loss_train: 0.7433 acc_train: 0.7714 loss_val: 0.6886 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0329 loss_train: 0.7939 acc_train: 0.8000 loss_val: 0.6881 acc_val: 0.8367 time: 0.0733s\n",
            "Epoch: 0330 loss_train: 0.6547 acc_train: 0.8357 loss_val: 0.6876 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0331 loss_train: 0.6090 acc_train: 0.8357 loss_val: 0.6871 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0332 loss_train: 0.6491 acc_train: 0.7929 loss_val: 0.6867 acc_val: 0.8367 time: 0.0735s\n",
            "Epoch: 0333 loss_train: 0.6498 acc_train: 0.8071 loss_val: 0.6864 acc_val: 0.8400 time: 0.0734s\n",
            "Epoch: 0334 loss_train: 0.6228 acc_train: 0.8214 loss_val: 0.6860 acc_val: 0.8400 time: 0.0733s\n",
            "Epoch: 0335 loss_train: 0.6800 acc_train: 0.8000 loss_val: 0.6857 acc_val: 0.8400 time: 0.0734s\n",
            "Epoch: 0336 loss_train: 0.7597 acc_train: 0.7571 loss_val: 0.6855 acc_val: 0.8400 time: 0.0738s\n",
            "Epoch: 0337 loss_train: 0.7259 acc_train: 0.7786 loss_val: 0.6853 acc_val: 0.8400 time: 0.0736s\n",
            "Epoch: 0338 loss_train: 0.7238 acc_train: 0.7786 loss_val: 0.6852 acc_val: 0.8367 time: 0.0733s\n",
            "Epoch: 0339 loss_train: 0.6226 acc_train: 0.7929 loss_val: 0.6850 acc_val: 0.8367 time: 0.0733s\n",
            "Epoch: 0340 loss_train: 0.5910 acc_train: 0.8500 loss_val: 0.6846 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0341 loss_train: 0.6302 acc_train: 0.8214 loss_val: 0.6844 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0342 loss_train: 0.6182 acc_train: 0.8143 loss_val: 0.6841 acc_val: 0.8367 time: 0.0733s\n",
            "Epoch: 0343 loss_train: 0.5665 acc_train: 0.8143 loss_val: 0.6839 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0344 loss_train: 0.6603 acc_train: 0.8000 loss_val: 0.6834 acc_val: 0.8367 time: 0.0735s\n",
            "Epoch: 0345 loss_train: 0.6278 acc_train: 0.8357 loss_val: 0.6829 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0346 loss_train: 0.6698 acc_train: 0.8214 loss_val: 0.6823 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0347 loss_train: 0.6648 acc_train: 0.8357 loss_val: 0.6818 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0348 loss_train: 0.7067 acc_train: 0.7786 loss_val: 0.6816 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0349 loss_train: 0.6568 acc_train: 0.7857 loss_val: 0.6812 acc_val: 0.8333 time: 0.0733s\n",
            "Epoch: 0350 loss_train: 0.6635 acc_train: 0.8071 loss_val: 0.6809 acc_val: 0.8333 time: 0.0735s\n",
            "Epoch: 0351 loss_train: 0.6802 acc_train: 0.7857 loss_val: 0.6807 acc_val: 0.8333 time: 0.0735s\n",
            "Epoch: 0352 loss_train: 0.6216 acc_train: 0.8429 loss_val: 0.6807 acc_val: 0.8333 time: 0.0735s\n",
            "Epoch: 0353 loss_train: 0.6611 acc_train: 0.8286 loss_val: 0.6807 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0354 loss_train: 0.6926 acc_train: 0.8071 loss_val: 0.6808 acc_val: 0.8367 time: 0.0733s\n",
            "Epoch: 0355 loss_train: 0.6254 acc_train: 0.8286 loss_val: 0.6808 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0356 loss_train: 0.5864 acc_train: 0.8143 loss_val: 0.6813 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0357 loss_train: 0.6684 acc_train: 0.8000 loss_val: 0.6817 acc_val: 0.8300 time: 0.0736s\n",
            "Epoch: 0358 loss_train: 0.5637 acc_train: 0.8500 loss_val: 0.6823 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0359 loss_train: 0.6788 acc_train: 0.8143 loss_val: 0.6828 acc_val: 0.8333 time: 0.0735s\n",
            "Epoch: 0360 loss_train: 0.6661 acc_train: 0.8143 loss_val: 0.6833 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0361 loss_train: 0.6562 acc_train: 0.7714 loss_val: 0.6838 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0362 loss_train: 0.6519 acc_train: 0.8214 loss_val: 0.6841 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0363 loss_train: 0.6524 acc_train: 0.8143 loss_val: 0.6843 acc_val: 0.8333 time: 0.0733s\n",
            "Epoch: 0364 loss_train: 0.6622 acc_train: 0.8143 loss_val: 0.6844 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0365 loss_train: 0.6722 acc_train: 0.8214 loss_val: 0.6843 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0366 loss_train: 0.5918 acc_train: 0.8286 loss_val: 0.6843 acc_val: 0.8300 time: 0.0741s\n",
            "Epoch: 0367 loss_train: 0.6337 acc_train: 0.7571 loss_val: 0.6842 acc_val: 0.8300 time: 0.0735s\n",
            "Epoch: 0368 loss_train: 0.6978 acc_train: 0.8071 loss_val: 0.6840 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0369 loss_train: 0.6604 acc_train: 0.8357 loss_val: 0.6838 acc_val: 0.8300 time: 0.0735s\n",
            "Epoch: 0370 loss_train: 0.6331 acc_train: 0.8429 loss_val: 0.6836 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0371 loss_train: 0.6729 acc_train: 0.8143 loss_val: 0.6834 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0372 loss_train: 0.6797 acc_train: 0.8214 loss_val: 0.6831 acc_val: 0.8300 time: 0.0737s\n",
            "Epoch: 0373 loss_train: 0.7140 acc_train: 0.7714 loss_val: 0.6832 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0374 loss_train: 0.5990 acc_train: 0.8500 loss_val: 0.6832 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0375 loss_train: 0.5877 acc_train: 0.8357 loss_val: 0.6830 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0376 loss_train: 0.6629 acc_train: 0.8071 loss_val: 0.6828 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0377 loss_train: 0.6498 acc_train: 0.7857 loss_val: 0.6828 acc_val: 0.8333 time: 0.0736s\n",
            "Epoch: 0378 loss_train: 0.6171 acc_train: 0.8429 loss_val: 0.6830 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0379 loss_train: 0.6562 acc_train: 0.8000 loss_val: 0.6828 acc_val: 0.8333 time: 0.0734s\n",
            "Epoch: 0380 loss_train: 0.6108 acc_train: 0.8357 loss_val: 0.6826 acc_val: 0.8333 time: 0.0733s\n",
            "Epoch: 0381 loss_train: 0.6866 acc_train: 0.7714 loss_val: 0.6824 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0382 loss_train: 0.5891 acc_train: 0.8286 loss_val: 0.6820 acc_val: 0.8367 time: 0.0734s\n",
            "Epoch: 0383 loss_train: 0.6366 acc_train: 0.8357 loss_val: 0.6818 acc_val: 0.8367 time: 0.0735s\n",
            "Epoch: 0384 loss_train: 0.6727 acc_train: 0.8000 loss_val: 0.6817 acc_val: 0.8367 time: 0.0735s\n",
            "Epoch: 0385 loss_train: 0.7192 acc_train: 0.8000 loss_val: 0.6813 acc_val: 0.8333 time: 0.0737s\n",
            "Epoch: 0386 loss_train: 0.5638 acc_train: 0.8214 loss_val: 0.6809 acc_val: 0.8333 time: 0.0735s\n",
            "Epoch: 0387 loss_train: 0.6736 acc_train: 0.8143 loss_val: 0.6807 acc_val: 0.8333 time: 0.0735s\n",
            "Epoch: 0388 loss_train: 0.6701 acc_train: 0.8286 loss_val: 0.6805 acc_val: 0.8333 time: 0.0735s\n",
            "Epoch: 0389 loss_train: 0.6949 acc_train: 0.8143 loss_val: 0.6802 acc_val: 0.8333 time: 0.0735s\n",
            "Epoch: 0390 loss_train: 0.6511 acc_train: 0.8071 loss_val: 0.6799 acc_val: 0.8300 time: 0.0733s\n",
            "Epoch: 0391 loss_train: 0.6998 acc_train: 0.8286 loss_val: 0.6796 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0392 loss_train: 0.5936 acc_train: 0.8429 loss_val: 0.6792 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0393 loss_train: 0.8840 acc_train: 0.7143 loss_val: 0.6788 acc_val: 0.8300 time: 0.0735s\n",
            "Epoch: 0394 loss_train: 0.6526 acc_train: 0.8071 loss_val: 0.6784 acc_val: 0.8300 time: 0.0735s\n",
            "Epoch: 0395 loss_train: 0.6293 acc_train: 0.8429 loss_val: 0.6782 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0396 loss_train: 0.5699 acc_train: 0.8286 loss_val: 0.6778 acc_val: 0.8267 time: 0.0736s\n",
            "Epoch: 0397 loss_train: 0.5794 acc_train: 0.8429 loss_val: 0.6772 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0398 loss_train: 0.6485 acc_train: 0.7857 loss_val: 0.6764 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0399 loss_train: 0.5800 acc_train: 0.8500 loss_val: 0.6755 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0400 loss_train: 0.6848 acc_train: 0.7571 loss_val: 0.6749 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0401 loss_train: 0.7706 acc_train: 0.7500 loss_val: 0.6746 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0402 loss_train: 0.6696 acc_train: 0.8214 loss_val: 0.6741 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0403 loss_train: 0.6923 acc_train: 0.7714 loss_val: 0.6738 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0404 loss_train: 0.6840 acc_train: 0.7929 loss_val: 0.6734 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0405 loss_train: 0.7177 acc_train: 0.7929 loss_val: 0.6731 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0406 loss_train: 0.6951 acc_train: 0.8000 loss_val: 0.6728 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0407 loss_train: 0.6579 acc_train: 0.8214 loss_val: 0.6726 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0408 loss_train: 0.5651 acc_train: 0.8286 loss_val: 0.6724 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0409 loss_train: 0.6661 acc_train: 0.8286 loss_val: 0.6723 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0410 loss_train: 0.6546 acc_train: 0.8143 loss_val: 0.6723 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0411 loss_train: 0.5778 acc_train: 0.8571 loss_val: 0.6723 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0412 loss_train: 0.6284 acc_train: 0.8429 loss_val: 0.6725 acc_val: 0.8200 time: 0.0733s\n",
            "Epoch: 0413 loss_train: 0.6675 acc_train: 0.7857 loss_val: 0.6727 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0414 loss_train: 0.6614 acc_train: 0.8286 loss_val: 0.6729 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0415 loss_train: 0.6988 acc_train: 0.8000 loss_val: 0.6730 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0416 loss_train: 0.7032 acc_train: 0.7857 loss_val: 0.6729 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0417 loss_train: 0.6130 acc_train: 0.8214 loss_val: 0.6727 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0418 loss_train: 0.5430 acc_train: 0.8500 loss_val: 0.6726 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0419 loss_train: 0.5312 acc_train: 0.8929 loss_val: 0.6724 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0420 loss_train: 0.6675 acc_train: 0.7643 loss_val: 0.6721 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0421 loss_train: 0.6349 acc_train: 0.8357 loss_val: 0.6719 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0422 loss_train: 0.6774 acc_train: 0.7929 loss_val: 0.6718 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0423 loss_train: 0.6786 acc_train: 0.7786 loss_val: 0.6716 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0424 loss_train: 0.5127 acc_train: 0.9071 loss_val: 0.6714 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0425 loss_train: 0.6712 acc_train: 0.8071 loss_val: 0.6713 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0426 loss_train: 0.5944 acc_train: 0.8714 loss_val: 0.6711 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0427 loss_train: 0.6642 acc_train: 0.8000 loss_val: 0.6709 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0428 loss_train: 0.6792 acc_train: 0.7929 loss_val: 0.6707 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0429 loss_train: 0.6835 acc_train: 0.8000 loss_val: 0.6707 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0430 loss_train: 0.6438 acc_train: 0.8071 loss_val: 0.6708 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0431 loss_train: 0.6367 acc_train: 0.7786 loss_val: 0.6709 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0432 loss_train: 0.6690 acc_train: 0.7643 loss_val: 0.6711 acc_val: 0.8233 time: 0.0746s\n",
            "Epoch: 0433 loss_train: 0.6178 acc_train: 0.8286 loss_val: 0.6713 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0434 loss_train: 0.5664 acc_train: 0.8286 loss_val: 0.6715 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0435 loss_train: 0.6508 acc_train: 0.8071 loss_val: 0.6717 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0436 loss_train: 0.6649 acc_train: 0.8286 loss_val: 0.6719 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0437 loss_train: 0.6243 acc_train: 0.8000 loss_val: 0.6723 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0438 loss_train: 0.6582 acc_train: 0.8000 loss_val: 0.6727 acc_val: 0.8100 time: 0.0734s\n",
            "Epoch: 0439 loss_train: 0.5963 acc_train: 0.8000 loss_val: 0.6730 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0440 loss_train: 0.6394 acc_train: 0.8071 loss_val: 0.6734 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0441 loss_train: 0.6112 acc_train: 0.8357 loss_val: 0.6738 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0442 loss_train: 0.6585 acc_train: 0.7786 loss_val: 0.6745 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0443 loss_train: 0.6392 acc_train: 0.8071 loss_val: 0.6751 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0444 loss_train: 0.6243 acc_train: 0.8214 loss_val: 0.6758 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0445 loss_train: 0.5951 acc_train: 0.8500 loss_val: 0.6764 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0446 loss_train: 0.5717 acc_train: 0.8571 loss_val: 0.6766 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0447 loss_train: 0.6960 acc_train: 0.7929 loss_val: 0.6767 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0448 loss_train: 0.6472 acc_train: 0.7929 loss_val: 0.6765 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0449 loss_train: 0.6362 acc_train: 0.8286 loss_val: 0.6760 acc_val: 0.8267 time: 0.0736s\n",
            "Epoch: 0450 loss_train: 0.5587 acc_train: 0.8571 loss_val: 0.6753 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0451 loss_train: 0.6680 acc_train: 0.7929 loss_val: 0.6745 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0452 loss_train: 0.6086 acc_train: 0.8357 loss_val: 0.6737 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0453 loss_train: 0.6359 acc_train: 0.8143 loss_val: 0.6730 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0454 loss_train: 0.6553 acc_train: 0.8000 loss_val: 0.6722 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0455 loss_train: 0.6266 acc_train: 0.8143 loss_val: 0.6720 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0456 loss_train: 0.7342 acc_train: 0.7929 loss_val: 0.6713 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0457 loss_train: 0.6245 acc_train: 0.7929 loss_val: 0.6710 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0458 loss_train: 0.5919 acc_train: 0.8214 loss_val: 0.6706 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0459 loss_train: 0.6235 acc_train: 0.8071 loss_val: 0.6702 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0460 loss_train: 0.6315 acc_train: 0.8071 loss_val: 0.6699 acc_val: 0.8233 time: 0.0733s\n",
            "Epoch: 0461 loss_train: 0.6234 acc_train: 0.8357 loss_val: 0.6697 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0462 loss_train: 0.6516 acc_train: 0.7929 loss_val: 0.6698 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0463 loss_train: 0.5750 acc_train: 0.8429 loss_val: 0.6698 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0464 loss_train: 0.6275 acc_train: 0.8214 loss_val: 0.6700 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0465 loss_train: 0.6751 acc_train: 0.8071 loss_val: 0.6703 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0466 loss_train: 0.6646 acc_train: 0.8071 loss_val: 0.6704 acc_val: 0.8167 time: 0.0737s\n",
            "Epoch: 0467 loss_train: 0.6312 acc_train: 0.8143 loss_val: 0.6704 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0468 loss_train: 0.6303 acc_train: 0.8071 loss_val: 0.6705 acc_val: 0.8067 time: 0.0734s\n",
            "Epoch: 0469 loss_train: 0.5669 acc_train: 0.8286 loss_val: 0.6710 acc_val: 0.8100 time: 0.0734s\n",
            "Epoch: 0470 loss_train: 0.6489 acc_train: 0.8000 loss_val: 0.6713 acc_val: 0.8100 time: 0.0734s\n",
            "Epoch: 0471 loss_train: 0.5945 acc_train: 0.8143 loss_val: 0.6717 acc_val: 0.8100 time: 0.0734s\n",
            "Epoch: 0472 loss_train: 0.7440 acc_train: 0.7929 loss_val: 0.6721 acc_val: 0.8100 time: 0.0734s\n",
            "Epoch: 0473 loss_train: 0.6356 acc_train: 0.8143 loss_val: 0.6725 acc_val: 0.8100 time: 0.0735s\n",
            "Epoch: 0474 loss_train: 0.5679 acc_train: 0.8429 loss_val: 0.6729 acc_val: 0.8100 time: 0.0735s\n",
            "Epoch: 0475 loss_train: 0.6103 acc_train: 0.8500 loss_val: 0.6736 acc_val: 0.8067 time: 0.0735s\n",
            "Epoch: 0476 loss_train: 0.5664 acc_train: 0.8214 loss_val: 0.6743 acc_val: 0.8067 time: 0.0735s\n",
            "Epoch: 0477 loss_train: 0.7460 acc_train: 0.7786 loss_val: 0.6750 acc_val: 0.8100 time: 0.0734s\n",
            "Epoch: 0478 loss_train: 0.7499 acc_train: 0.7786 loss_val: 0.6755 acc_val: 0.8100 time: 0.0734s\n",
            "Epoch: 0479 loss_train: 0.6456 acc_train: 0.7929 loss_val: 0.6760 acc_val: 0.8133 time: 0.0734s\n",
            "Epoch: 0480 loss_train: 0.6549 acc_train: 0.8214 loss_val: 0.6762 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0481 loss_train: 0.5980 acc_train: 0.8643 loss_val: 0.6764 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0482 loss_train: 0.6135 acc_train: 0.8071 loss_val: 0.6762 acc_val: 0.8133 time: 0.0735s\n",
            "Epoch: 0483 loss_train: 0.6909 acc_train: 0.8000 loss_val: 0.6762 acc_val: 0.8133 time: 0.0734s\n",
            "Epoch: 0484 loss_train: 0.6875 acc_train: 0.8000 loss_val: 0.6764 acc_val: 0.8133 time: 0.0734s\n",
            "Epoch: 0485 loss_train: 0.5728 acc_train: 0.7929 loss_val: 0.6765 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0486 loss_train: 0.6078 acc_train: 0.8571 loss_val: 0.6765 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0487 loss_train: 0.7077 acc_train: 0.7857 loss_val: 0.6768 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0488 loss_train: 0.7125 acc_train: 0.8214 loss_val: 0.6769 acc_val: 0.8167 time: 0.0733s\n",
            "Epoch: 0489 loss_train: 0.6534 acc_train: 0.8071 loss_val: 0.6769 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0490 loss_train: 0.6656 acc_train: 0.7786 loss_val: 0.6765 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0491 loss_train: 0.6517 acc_train: 0.7714 loss_val: 0.6759 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0492 loss_train: 0.6673 acc_train: 0.8000 loss_val: 0.6750 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0493 loss_train: 0.6206 acc_train: 0.8214 loss_val: 0.6742 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0494 loss_train: 0.6123 acc_train: 0.8500 loss_val: 0.6734 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0495 loss_train: 0.6205 acc_train: 0.8357 loss_val: 0.6726 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0496 loss_train: 0.5658 acc_train: 0.8286 loss_val: 0.6717 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0497 loss_train: 0.6561 acc_train: 0.8571 loss_val: 0.6707 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0498 loss_train: 0.6273 acc_train: 0.8143 loss_val: 0.6697 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0499 loss_train: 0.7059 acc_train: 0.7571 loss_val: 0.6690 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0500 loss_train: 0.6125 acc_train: 0.8357 loss_val: 0.6684 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0501 loss_train: 0.5341 acc_train: 0.8214 loss_val: 0.6680 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0502 loss_train: 0.5944 acc_train: 0.8143 loss_val: 0.6676 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0503 loss_train: 0.6892 acc_train: 0.7786 loss_val: 0.6670 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0504 loss_train: 0.6289 acc_train: 0.8071 loss_val: 0.6664 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0505 loss_train: 0.5562 acc_train: 0.8500 loss_val: 0.6661 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0506 loss_train: 0.6298 acc_train: 0.8214 loss_val: 0.6659 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0507 loss_train: 0.5861 acc_train: 0.8357 loss_val: 0.6657 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0508 loss_train: 0.5393 acc_train: 0.8857 loss_val: 0.6651 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0509 loss_train: 0.6223 acc_train: 0.8286 loss_val: 0.6646 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0510 loss_train: 0.6491 acc_train: 0.7929 loss_val: 0.6640 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0511 loss_train: 0.6056 acc_train: 0.8214 loss_val: 0.6635 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0512 loss_train: 0.6731 acc_train: 0.8357 loss_val: 0.6633 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0513 loss_train: 0.6406 acc_train: 0.8214 loss_val: 0.6628 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0514 loss_train: 0.5822 acc_train: 0.8000 loss_val: 0.6625 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0515 loss_train: 0.6761 acc_train: 0.7929 loss_val: 0.6629 acc_val: 0.8133 time: 0.0734s\n",
            "Epoch: 0516 loss_train: 0.6009 acc_train: 0.8143 loss_val: 0.6628 acc_val: 0.8133 time: 0.0734s\n",
            "Epoch: 0517 loss_train: 0.5814 acc_train: 0.8500 loss_val: 0.6627 acc_val: 0.8133 time: 0.0733s\n",
            "Epoch: 0518 loss_train: 0.7241 acc_train: 0.7786 loss_val: 0.6623 acc_val: 0.8133 time: 0.0735s\n",
            "Epoch: 0519 loss_train: 0.5752 acc_train: 0.8357 loss_val: 0.6619 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0520 loss_train: 0.6642 acc_train: 0.8000 loss_val: 0.6614 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0521 loss_train: 0.6404 acc_train: 0.8286 loss_val: 0.6607 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0522 loss_train: 0.6358 acc_train: 0.8357 loss_val: 0.6602 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0523 loss_train: 0.5638 acc_train: 0.7929 loss_val: 0.6596 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0524 loss_train: 0.5918 acc_train: 0.8286 loss_val: 0.6589 acc_val: 0.8167 time: 0.0733s\n",
            "Epoch: 0525 loss_train: 0.6673 acc_train: 0.8071 loss_val: 0.6583 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0526 loss_train: 0.5134 acc_train: 0.8786 loss_val: 0.6579 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0527 loss_train: 0.6243 acc_train: 0.8143 loss_val: 0.6575 acc_val: 0.8167 time: 0.0737s\n",
            "Epoch: 0528 loss_train: 0.5527 acc_train: 0.8500 loss_val: 0.6573 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0529 loss_train: 0.6917 acc_train: 0.7786 loss_val: 0.6573 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0530 loss_train: 0.6550 acc_train: 0.8000 loss_val: 0.6573 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0531 loss_train: 0.6575 acc_train: 0.7857 loss_val: 0.6572 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0532 loss_train: 0.6751 acc_train: 0.7857 loss_val: 0.6572 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0533 loss_train: 0.6306 acc_train: 0.8214 loss_val: 0.6572 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0534 loss_train: 0.5674 acc_train: 0.8500 loss_val: 0.6568 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0535 loss_train: 0.7225 acc_train: 0.7786 loss_val: 0.6563 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0536 loss_train: 0.5319 acc_train: 0.8714 loss_val: 0.6560 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0537 loss_train: 0.6751 acc_train: 0.7786 loss_val: 0.6558 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0538 loss_train: 0.6327 acc_train: 0.7929 loss_val: 0.6556 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0539 loss_train: 0.6490 acc_train: 0.8000 loss_val: 0.6552 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0540 loss_train: 0.5474 acc_train: 0.8500 loss_val: 0.6551 acc_val: 0.8300 time: 0.0735s\n",
            "Epoch: 0541 loss_train: 0.6001 acc_train: 0.8286 loss_val: 0.6549 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0542 loss_train: 0.6548 acc_train: 0.8000 loss_val: 0.6548 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0543 loss_train: 0.6269 acc_train: 0.8143 loss_val: 0.6549 acc_val: 0.8300 time: 0.0735s\n",
            "Epoch: 0544 loss_train: 0.6595 acc_train: 0.8000 loss_val: 0.6552 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0545 loss_train: 0.6360 acc_train: 0.7929 loss_val: 0.6554 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0546 loss_train: 0.6891 acc_train: 0.7857 loss_val: 0.6557 acc_val: 0.8267 time: 0.0733s\n",
            "Epoch: 0547 loss_train: 0.6581 acc_train: 0.7857 loss_val: 0.6559 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0548 loss_train: 0.5626 acc_train: 0.8357 loss_val: 0.6566 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0549 loss_train: 0.6790 acc_train: 0.8214 loss_val: 0.6572 acc_val: 0.8267 time: 0.0736s\n",
            "Epoch: 0550 loss_train: 0.6626 acc_train: 0.8000 loss_val: 0.6579 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0551 loss_train: 0.6496 acc_train: 0.8214 loss_val: 0.6585 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0552 loss_train: 0.6797 acc_train: 0.7786 loss_val: 0.6587 acc_val: 0.8267 time: 0.0762s\n",
            "Epoch: 0553 loss_train: 0.6476 acc_train: 0.7929 loss_val: 0.6587 acc_val: 0.8300 time: 0.0735s\n",
            "Epoch: 0554 loss_train: 0.7380 acc_train: 0.7857 loss_val: 0.6589 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0555 loss_train: 0.5453 acc_train: 0.8071 loss_val: 0.6589 acc_val: 0.8300 time: 0.0735s\n",
            "Epoch: 0556 loss_train: 0.6561 acc_train: 0.7929 loss_val: 0.6588 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0557 loss_train: 0.6318 acc_train: 0.8143 loss_val: 0.6585 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0558 loss_train: 0.6746 acc_train: 0.7786 loss_val: 0.6581 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0559 loss_train: 0.5958 acc_train: 0.8500 loss_val: 0.6581 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0560 loss_train: 0.6151 acc_train: 0.8143 loss_val: 0.6579 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0561 loss_train: 0.4956 acc_train: 0.8500 loss_val: 0.6577 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0562 loss_train: 0.6359 acc_train: 0.8143 loss_val: 0.6574 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0563 loss_train: 0.7035 acc_train: 0.7929 loss_val: 0.6566 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0564 loss_train: 0.5682 acc_train: 0.8357 loss_val: 0.6562 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0565 loss_train: 0.6314 acc_train: 0.8214 loss_val: 0.6558 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0566 loss_train: 0.6104 acc_train: 0.8286 loss_val: 0.6555 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0567 loss_train: 0.5304 acc_train: 0.8571 loss_val: 0.6552 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0568 loss_train: 0.6871 acc_train: 0.7643 loss_val: 0.6550 acc_val: 0.8267 time: 0.0736s\n",
            "Epoch: 0569 loss_train: 0.5622 acc_train: 0.8357 loss_val: 0.6548 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0570 loss_train: 0.5540 acc_train: 0.8571 loss_val: 0.6545 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0571 loss_train: 0.6252 acc_train: 0.8071 loss_val: 0.6542 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0572 loss_train: 0.7042 acc_train: 0.7786 loss_val: 0.6539 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0573 loss_train: 0.6380 acc_train: 0.8214 loss_val: 0.6536 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0574 loss_train: 0.7443 acc_train: 0.7857 loss_val: 0.6539 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0575 loss_train: 0.6821 acc_train: 0.7857 loss_val: 0.6540 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0576 loss_train: 0.5152 acc_train: 0.8571 loss_val: 0.6542 acc_val: 0.8200 time: 0.0737s\n",
            "Epoch: 0577 loss_train: 0.6326 acc_train: 0.7643 loss_val: 0.6545 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0578 loss_train: 0.6143 acc_train: 0.8286 loss_val: 0.6549 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0579 loss_train: 0.6654 acc_train: 0.7857 loss_val: 0.6555 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0580 loss_train: 0.6678 acc_train: 0.7929 loss_val: 0.6558 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0581 loss_train: 0.5965 acc_train: 0.8500 loss_val: 0.6562 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0582 loss_train: 0.5995 acc_train: 0.8286 loss_val: 0.6565 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0583 loss_train: 0.6572 acc_train: 0.7643 loss_val: 0.6568 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0584 loss_train: 0.6081 acc_train: 0.8000 loss_val: 0.6569 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0585 loss_train: 0.5742 acc_train: 0.8571 loss_val: 0.6571 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0586 loss_train: 0.6687 acc_train: 0.8000 loss_val: 0.6576 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0587 loss_train: 0.6043 acc_train: 0.8286 loss_val: 0.6578 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0588 loss_train: 0.5379 acc_train: 0.8643 loss_val: 0.6578 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0589 loss_train: 0.5393 acc_train: 0.8429 loss_val: 0.6580 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0590 loss_train: 0.5767 acc_train: 0.8143 loss_val: 0.6582 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0591 loss_train: 0.6450 acc_train: 0.7929 loss_val: 0.6585 acc_val: 0.8167 time: 0.0736s\n",
            "Epoch: 0592 loss_train: 0.6092 acc_train: 0.8214 loss_val: 0.6588 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0593 loss_train: 0.6502 acc_train: 0.8000 loss_val: 0.6587 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0594 loss_train: 0.6314 acc_train: 0.8429 loss_val: 0.6584 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0595 loss_train: 0.6110 acc_train: 0.7929 loss_val: 0.6581 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0596 loss_train: 0.6382 acc_train: 0.8000 loss_val: 0.6578 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0597 loss_train: 0.6572 acc_train: 0.7929 loss_val: 0.6575 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0598 loss_train: 0.5652 acc_train: 0.8357 loss_val: 0.6570 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0599 loss_train: 0.6654 acc_train: 0.8000 loss_val: 0.6565 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0600 loss_train: 0.5552 acc_train: 0.8429 loss_val: 0.6562 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0601 loss_train: 0.5273 acc_train: 0.8786 loss_val: 0.6559 acc_val: 0.8200 time: 0.0736s\n",
            "Epoch: 0602 loss_train: 0.6138 acc_train: 0.8071 loss_val: 0.6553 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0603 loss_train: 0.6784 acc_train: 0.7643 loss_val: 0.6550 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0604 loss_train: 0.5982 acc_train: 0.8357 loss_val: 0.6548 acc_val: 0.8200 time: 0.0736s\n",
            "Epoch: 0605 loss_train: 0.6028 acc_train: 0.8571 loss_val: 0.6546 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0606 loss_train: 0.6201 acc_train: 0.8286 loss_val: 0.6544 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0607 loss_train: 0.5492 acc_train: 0.8714 loss_val: 0.6541 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0608 loss_train: 0.6027 acc_train: 0.8143 loss_val: 0.6537 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0609 loss_train: 0.5658 acc_train: 0.8357 loss_val: 0.6532 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0610 loss_train: 0.5738 acc_train: 0.8429 loss_val: 0.6526 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0611 loss_train: 0.7431 acc_train: 0.7571 loss_val: 0.6522 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0612 loss_train: 0.6344 acc_train: 0.8143 loss_val: 0.6520 acc_val: 0.8233 time: 0.0736s\n",
            "Epoch: 0613 loss_train: 0.5542 acc_train: 0.8429 loss_val: 0.6520 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0614 loss_train: 0.5518 acc_train: 0.8500 loss_val: 0.6519 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0615 loss_train: 0.5504 acc_train: 0.8500 loss_val: 0.6516 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0616 loss_train: 0.6651 acc_train: 0.7857 loss_val: 0.6515 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0617 loss_train: 0.4755 acc_train: 0.8786 loss_val: 0.6512 acc_val: 0.8233 time: 0.0736s\n",
            "Epoch: 0618 loss_train: 0.5168 acc_train: 0.8643 loss_val: 0.6512 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0619 loss_train: 0.6343 acc_train: 0.7929 loss_val: 0.6513 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0620 loss_train: 0.6484 acc_train: 0.8214 loss_val: 0.6517 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0621 loss_train: 0.6903 acc_train: 0.7929 loss_val: 0.6521 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0622 loss_train: 0.5694 acc_train: 0.8500 loss_val: 0.6525 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0623 loss_train: 0.7114 acc_train: 0.7643 loss_val: 0.6530 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0624 loss_train: 0.5522 acc_train: 0.8643 loss_val: 0.6532 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0625 loss_train: 0.5463 acc_train: 0.8786 loss_val: 0.6534 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0626 loss_train: 0.6480 acc_train: 0.8071 loss_val: 0.6537 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0627 loss_train: 0.6507 acc_train: 0.7857 loss_val: 0.6541 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0628 loss_train: 0.5314 acc_train: 0.8786 loss_val: 0.6546 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0629 loss_train: 0.6430 acc_train: 0.8071 loss_val: 0.6554 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0630 loss_train: 0.7129 acc_train: 0.7500 loss_val: 0.6562 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0631 loss_train: 0.4641 acc_train: 0.8786 loss_val: 0.6567 acc_val: 0.8233 time: 0.0736s\n",
            "Epoch: 0632 loss_train: 0.6126 acc_train: 0.8357 loss_val: 0.6568 acc_val: 0.8233 time: 0.0736s\n",
            "Epoch: 0633 loss_train: 0.6203 acc_train: 0.8214 loss_val: 0.6571 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0634 loss_train: 0.6300 acc_train: 0.8071 loss_val: 0.6572 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0635 loss_train: 0.6113 acc_train: 0.8286 loss_val: 0.6574 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0636 loss_train: 0.5646 acc_train: 0.8214 loss_val: 0.6574 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0637 loss_train: 0.6753 acc_train: 0.7786 loss_val: 0.6573 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0638 loss_train: 0.6389 acc_train: 0.7643 loss_val: 0.6575 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0639 loss_train: 0.5603 acc_train: 0.8500 loss_val: 0.6576 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0640 loss_train: 0.6252 acc_train: 0.8143 loss_val: 0.6577 acc_val: 0.8233 time: 0.0736s\n",
            "Epoch: 0641 loss_train: 0.6620 acc_train: 0.8000 loss_val: 0.6580 acc_val: 0.8233 time: 0.0736s\n",
            "Epoch: 0642 loss_train: 0.6246 acc_train: 0.8000 loss_val: 0.6585 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0643 loss_train: 0.6864 acc_train: 0.8286 loss_val: 0.6589 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0644 loss_train: 0.6029 acc_train: 0.8357 loss_val: 0.6589 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0645 loss_train: 0.5642 acc_train: 0.8500 loss_val: 0.6587 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0646 loss_train: 0.5729 acc_train: 0.8429 loss_val: 0.6587 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0647 loss_train: 0.5300 acc_train: 0.8500 loss_val: 0.6586 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0648 loss_train: 0.6086 acc_train: 0.8000 loss_val: 0.6583 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0649 loss_train: 0.5998 acc_train: 0.8357 loss_val: 0.6578 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0650 loss_train: 0.5944 acc_train: 0.8429 loss_val: 0.6575 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0651 loss_train: 0.5717 acc_train: 0.8286 loss_val: 0.6570 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0652 loss_train: 0.6648 acc_train: 0.7929 loss_val: 0.6563 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0653 loss_train: 0.7124 acc_train: 0.7929 loss_val: 0.6556 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0654 loss_train: 0.6602 acc_train: 0.7929 loss_val: 0.6552 acc_val: 0.8200 time: 0.0736s\n",
            "Epoch: 0655 loss_train: 0.5917 acc_train: 0.8214 loss_val: 0.6553 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0656 loss_train: 0.6623 acc_train: 0.8143 loss_val: 0.6556 acc_val: 0.8133 time: 0.0735s\n",
            "Epoch: 0657 loss_train: 0.6091 acc_train: 0.8143 loss_val: 0.6561 acc_val: 0.8133 time: 0.0735s\n",
            "Epoch: 0658 loss_train: 0.6480 acc_train: 0.8214 loss_val: 0.6567 acc_val: 0.8133 time: 0.0735s\n",
            "Epoch: 0659 loss_train: 0.7304 acc_train: 0.7786 loss_val: 0.6571 acc_val: 0.8100 time: 0.0734s\n",
            "Epoch: 0660 loss_train: 0.6118 acc_train: 0.8000 loss_val: 0.6576 acc_val: 0.8100 time: 0.0738s\n",
            "Epoch: 0661 loss_train: 0.5724 acc_train: 0.8143 loss_val: 0.6579 acc_val: 0.8100 time: 0.0734s\n",
            "Epoch: 0662 loss_train: 0.5605 acc_train: 0.8357 loss_val: 0.6581 acc_val: 0.8100 time: 0.0734s\n",
            "Epoch: 0663 loss_train: 0.5759 acc_train: 0.8429 loss_val: 0.6582 acc_val: 0.8100 time: 0.0735s\n",
            "Epoch: 0664 loss_train: 0.6254 acc_train: 0.8214 loss_val: 0.6583 acc_val: 0.8100 time: 0.0734s\n",
            "Epoch: 0665 loss_train: 0.5461 acc_train: 0.8571 loss_val: 0.6583 acc_val: 0.8100 time: 0.0734s\n",
            "Epoch: 0666 loss_train: 0.5912 acc_train: 0.8357 loss_val: 0.6580 acc_val: 0.8100 time: 0.0735s\n",
            "Epoch: 0667 loss_train: 0.5772 acc_train: 0.8357 loss_val: 0.6579 acc_val: 0.8100 time: 0.0735s\n",
            "Epoch: 0668 loss_train: 0.5881 acc_train: 0.8143 loss_val: 0.6576 acc_val: 0.8100 time: 0.0735s\n",
            "Epoch: 0669 loss_train: 0.5910 acc_train: 0.8143 loss_val: 0.6572 acc_val: 0.8100 time: 0.0735s\n",
            "Epoch: 0670 loss_train: 0.5294 acc_train: 0.8571 loss_val: 0.6568 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0671 loss_train: 0.6458 acc_train: 0.7643 loss_val: 0.6563 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0672 loss_train: 0.5741 acc_train: 0.8571 loss_val: 0.6563 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0673 loss_train: 0.5568 acc_train: 0.8286 loss_val: 0.6566 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0674 loss_train: 0.6350 acc_train: 0.7929 loss_val: 0.6567 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0675 loss_train: 0.5922 acc_train: 0.8429 loss_val: 0.6569 acc_val: 0.8233 time: 0.0736s\n",
            "Epoch: 0676 loss_train: 0.5214 acc_train: 0.8500 loss_val: 0.6571 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0677 loss_train: 0.5714 acc_train: 0.8643 loss_val: 0.6572 acc_val: 0.8233 time: 0.0734s\n",
            "Epoch: 0678 loss_train: 0.6063 acc_train: 0.8000 loss_val: 0.6574 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0679 loss_train: 0.6239 acc_train: 0.8071 loss_val: 0.6577 acc_val: 0.8267 time: 0.0736s\n",
            "Epoch: 0680 loss_train: 0.5992 acc_train: 0.8500 loss_val: 0.6580 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0681 loss_train: 0.6340 acc_train: 0.7929 loss_val: 0.6583 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0682 loss_train: 0.5761 acc_train: 0.8500 loss_val: 0.6586 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0683 loss_train: 0.5393 acc_train: 0.8429 loss_val: 0.6589 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0684 loss_train: 0.6477 acc_train: 0.8286 loss_val: 0.6590 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0685 loss_train: 0.6285 acc_train: 0.8571 loss_val: 0.6592 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0686 loss_train: 0.6385 acc_train: 0.8429 loss_val: 0.6589 acc_val: 0.8267 time: 0.0734s\n",
            "Epoch: 0687 loss_train: 0.6081 acc_train: 0.8000 loss_val: 0.6586 acc_val: 0.8300 time: 0.0735s\n",
            "Epoch: 0688 loss_train: 0.6802 acc_train: 0.7857 loss_val: 0.6580 acc_val: 0.8300 time: 0.0735s\n",
            "Epoch: 0689 loss_train: 0.5380 acc_train: 0.8786 loss_val: 0.6573 acc_val: 0.8300 time: 0.0735s\n",
            "Epoch: 0690 loss_train: 0.5949 acc_train: 0.8214 loss_val: 0.6566 acc_val: 0.8300 time: 0.0735s\n",
            "Epoch: 0691 loss_train: 0.5502 acc_train: 0.8429 loss_val: 0.6560 acc_val: 0.8300 time: 0.0735s\n",
            "Epoch: 0692 loss_train: 0.6266 acc_train: 0.8214 loss_val: 0.6555 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0693 loss_train: 0.6444 acc_train: 0.8429 loss_val: 0.6553 acc_val: 0.8300 time: 0.0734s\n",
            "Epoch: 0694 loss_train: 0.4926 acc_train: 0.8571 loss_val: 0.6547 acc_val: 0.8300 time: 0.0735s\n",
            "Epoch: 0695 loss_train: 0.5167 acc_train: 0.8357 loss_val: 0.6543 acc_val: 0.8267 time: 0.0735s\n",
            "Epoch: 0696 loss_train: 0.6318 acc_train: 0.8357 loss_val: 0.6541 acc_val: 0.8233 time: 0.0735s\n",
            "Epoch: 0697 loss_train: 0.6173 acc_train: 0.8500 loss_val: 0.6539 acc_val: 0.8200 time: 0.0734s\n",
            "Epoch: 0698 loss_train: 0.7201 acc_train: 0.7571 loss_val: 0.6537 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0699 loss_train: 0.6611 acc_train: 0.8071 loss_val: 0.6535 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0700 loss_train: 0.6430 acc_train: 0.8143 loss_val: 0.6532 acc_val: 0.8167 time: 0.0736s\n",
            "Epoch: 0701 loss_train: 0.7380 acc_train: 0.7714 loss_val: 0.6528 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0702 loss_train: 0.6187 acc_train: 0.7929 loss_val: 0.6523 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0703 loss_train: 0.6422 acc_train: 0.7857 loss_val: 0.6522 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0704 loss_train: 0.5664 acc_train: 0.8286 loss_val: 0.6521 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0705 loss_train: 0.5104 acc_train: 0.8714 loss_val: 0.6522 acc_val: 0.8167 time: 0.0736s\n",
            "Epoch: 0706 loss_train: 0.5663 acc_train: 0.8571 loss_val: 0.6520 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0707 loss_train: 0.6045 acc_train: 0.7929 loss_val: 0.6519 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0708 loss_train: 0.6252 acc_train: 0.8143 loss_val: 0.6519 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0709 loss_train: 0.5904 acc_train: 0.8143 loss_val: 0.6519 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0710 loss_train: 0.6438 acc_train: 0.8000 loss_val: 0.6522 acc_val: 0.8133 time: 0.0735s\n",
            "Epoch: 0711 loss_train: 0.5788 acc_train: 0.8429 loss_val: 0.6523 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0712 loss_train: 0.6385 acc_train: 0.8214 loss_val: 0.6523 acc_val: 0.8167 time: 0.0734s\n",
            "Epoch: 0713 loss_train: 0.6275 acc_train: 0.8214 loss_val: 0.6525 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0714 loss_train: 0.6190 acc_train: 0.8357 loss_val: 0.6529 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0715 loss_train: 0.5708 acc_train: 0.8429 loss_val: 0.6534 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0716 loss_train: 0.6111 acc_train: 0.7857 loss_val: 0.6538 acc_val: 0.8200 time: 0.0735s\n",
            "Epoch: 0717 loss_train: 0.5201 acc_train: 0.8500 loss_val: 0.6542 acc_val: 0.8167 time: 0.0735s\n",
            "Epoch: 0718 loss_train: 0.7117 acc_train: 0.7500 loss_val: 0.6547 acc_val: 0.8167 time: 0.0735s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Restore best model\n",
        "print('Loading {}th epoch'.format(best_epoch))\n",
        "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "# Testing\n",
        "compute_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfbGYEAjXiZR",
        "outputId": "bfe89b3e-7c26-4a00-c408-83be262520fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization Finished!\n",
            "Total time elapsed: 59.3218s\n",
            "Loading 617th epoch\n",
            "Test set results: loss= 0.6626 accuracy= 0.8410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# visualization"
      ],
      "metadata": {
        "id": "bj6fcGWcZhM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "import torch\n",
        "# import models\n",
        "\n",
        "def make_dot(var, params):\n",
        "    \"\"\" Produces Graphviz representation of PyTorch autograd graph\n",
        "    \n",
        "    Blue nodes are the Variables that require grad, orange are Tensors\n",
        "    saved for backward in torch.autograd.Function\n",
        "    \n",
        "    Args:\n",
        "        var: output Variable\n",
        "        params: dict of (name, Variable) to add names to node that\n",
        "            require grad (TODO: make optional)\n",
        "    \"\"\"\n",
        "    param_map = {id(v): k for k, v in params.items()}\n",
        "    print(param_map)\n",
        "    \n",
        "    node_attr = dict(style='filled',\n",
        "                     shape='box',\n",
        "                     align='left',\n",
        "                     fontsize='12',\n",
        "                     ranksep='0.1',\n",
        "                     height='0.2')\n",
        "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
        "    seen = set()\n",
        "    \n",
        "    def size_to_str(size):\n",
        "        return '('+(', ').join(['%d'% v for v in size])+')'\n",
        "\n",
        "    def add_nodes(var):\n",
        "        if var not in seen:\n",
        "            if torch.is_tensor(var):\n",
        "                dot.node(str(id(var)), size_to_str(var.size()), fillcolor='orange')\n",
        "            elif hasattr(var, 'variable'):\n",
        "                u = var.variable\n",
        "                node_name = '%s\\n %s' % (param_map.get(id(u)), size_to_str(u.size()))\n",
        "                dot.node(str(id(var)), node_name, fillcolor='lightblue')\n",
        "            else:\n",
        "                dot.node(str(id(var)), str(type(var).__name__))\n",
        "            seen.add(var)\n",
        "            if hasattr(var, 'next_functions'):\n",
        "                for u in var.next_functions:\n",
        "                    if u[0] is not None:\n",
        "                        dot.edge(str(id(u[0])), str(id(var)))\n",
        "                        add_nodes(u[0])\n",
        "            if hasattr(var, 'saved_tensors'):\n",
        "                for t in var.saved_tensors:\n",
        "                    dot.edge(str(id(t)), str(id(var)))\n",
        "                    add_nodes(t)\n",
        "    add_nodes(var.grad_fn)\n",
        "    return dot\n",
        "    "
      ],
      "metadata": {
        "id": "5VwsWb9NZjXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.randn(100, 50).cuda()\n",
        "adj = torch.randn(100, 100).cuda()\n",
        "model = SpGAT(50, 8, 7, 0.5, 0.01, 3)\n",
        "model = model.cuda()\n",
        "y = model(inputs, adj)\n",
        "\n",
        "g = make_dot(y, model.state_dict())\n",
        "g.view()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "T3SVOR2IaKn5",
        "outputId": "23efe3f1-6685-4d9d-ffdd-3505f1a81315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{139794002429248: 'attention_0.W', 139794002431488: 'attention_0.a', 139794002429888: 'attention_1.W', 139794002429168: 'attention_1.a', 139794002431168: 'attention_2.W', 139794002428128: 'attention_2.a', 139794002428288: 'out_att.W', 139794002396320: 'out_att.a'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Digraph.gv.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ]
}