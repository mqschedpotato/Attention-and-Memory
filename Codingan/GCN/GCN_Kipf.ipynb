{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuqnDry5jEtM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot"
      ],
      "metadata": {
        "id": "srqXYsIqjOuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path=\"/content/sample_data/cora\", dataset=\"cora\"):\n",
        "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
        "    print('Loading {} dataset...'.format(dataset))\n",
        "\n",
        "    idx_features_labels = np.genfromtxt(\"{}.content\".format(path, dataset),\n",
        "                                        dtype=np.dtype(str))\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "\n",
        "    # build graph\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    edges_unordered = np.genfromtxt(\"{}.cites\".format(path, dataset),\n",
        "                                    dtype=np.int32)\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # build symmetric adjacency matrix\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    features = normalize(features)\n",
        "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "    idx_train = range(140)\n",
        "    idx_val = range(200, 500)\n",
        "    idx_test = range(500, 1500)\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test"
      ],
      "metadata": {
        "id": "OTgIw_dXjQt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ],
      "metadata": {
        "id": "au0LxvYgjXF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)"
      ],
      "metadata": {
        "id": "Ma63_2t_jZvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)"
      ],
      "metadata": {
        "id": "TA4oUCUhjbuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module"
      ],
      "metadata": {
        "id": "IXo9pjwajddm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'"
      ],
      "metadata": {
        "id": "fw9kFOsDjh-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "H9jMUsKhjlaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "kKw4RhE3jqj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "n06Yop9ajuIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
        "                    help='Disables CUDA training.')\n",
        "parser.add_argument('--fastmode', action='store_true', default=False,\n",
        "                    help='Validate during training pass.')\n",
        "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=200,\n",
        "                    help='Number of epochs to train.')\n",
        "parser.add_argument('--lr', type=float, default=0.01,\n",
        "                    help='Initial learning rate.')\n",
        "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
        "                    help='Weight decay (L2 loss on parameters).')\n",
        "parser.add_argument('--hidden', type=int, default=16,\n",
        "                    help='Number of hidden units.')\n",
        "parser.add_argument('--dropout', type=float, default=0.5,\n",
        "                    help='Dropout rate (1 - keep probability).')"
      ],
      "metadata": {
        "id": "7Wg0DrrLjyQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a6a07bb-f825-43ae-ff4d-cbc544b545d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--dropout'], dest='dropout', nargs=None, const=None, default=0.5, type=<class 'float'>, choices=None, help='Dropout rate (1 - keep probability).', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args, unknown = parser.parse_known_args()\n",
        "args.cuda = not args.no_cuda and torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "bESprWpb1D_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed()\n",
        "torch.manual_seed(args.seed)\n",
        "if args.cuda:\n",
        "    torch.cuda.manual_seed(args.seed)"
      ],
      "metadata": {
        "id": "oLtWprtqj0u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
      ],
      "metadata": {
        "id": "erlN_IV8j2h_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20cd4e6a-3061-4b41-82fb-f0404f54a925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading cora dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN(nfeat=features.shape[1],\n",
        "            nhid=args.hidden,\n",
        "            nclass=labels.max().item() + 1,\n",
        "            dropout=args.dropout)\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=args.lr, weight_decay=args.weight_decay)"
      ],
      "metadata": {
        "id": "Bh1jfuOKj4PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if args.cuda:\n",
        "    model.cuda()\n",
        "    features = features.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()"
      ],
      "metadata": {
        "id": "wMY6t_DQj8T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(features, adj)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if not args.fastmode:\n",
        "        # Evaluate validation set performance separately,\n",
        "        # deactivates dropout during validation run.\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))"
      ],
      "metadata": {
        "id": "R5atMYZCj8yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.item()))"
      ],
      "metadata": {
        "id": "sAkdxIgIj_mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "t_total = time.time()\n",
        "for epoch in range(args.epochs):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
      ],
      "metadata": {
        "id": "u0HvZloRkCMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f866e2a-1fcc-467c-f024-d4c3490c58be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9282 acc_train: 0.1357 loss_val: 1.9326 acc_val: 0.1200 time: 2.3885s\n",
            "Epoch: 0002 loss_train: 1.9233 acc_train: 0.1500 loss_val: 1.9201 acc_val: 0.1767 time: 0.0057s\n",
            "Epoch: 0003 loss_train: 1.9092 acc_train: 0.1857 loss_val: 1.9081 acc_val: 0.1433 time: 0.0052s\n",
            "Epoch: 0004 loss_train: 1.8817 acc_train: 0.2714 loss_val: 1.8962 acc_val: 0.1567 time: 0.0051s\n",
            "Epoch: 0005 loss_train: 1.8785 acc_train: 0.2214 loss_val: 1.8847 acc_val: 0.1567 time: 0.0050s\n",
            "Epoch: 0006 loss_train: 1.8722 acc_train: 0.2071 loss_val: 1.8735 acc_val: 0.1567 time: 0.0069s\n",
            "Epoch: 0007 loss_train: 1.8642 acc_train: 0.2214 loss_val: 1.8625 acc_val: 0.1567 time: 0.0076s\n",
            "Epoch: 0008 loss_train: 1.8493 acc_train: 0.2214 loss_val: 1.8520 acc_val: 0.1567 time: 0.0051s\n",
            "Epoch: 0009 loss_train: 1.8461 acc_train: 0.2357 loss_val: 1.8420 acc_val: 0.2100 time: 0.0051s\n",
            "Epoch: 0010 loss_train: 1.8331 acc_train: 0.2429 loss_val: 1.8323 acc_val: 0.4400 time: 0.0050s\n",
            "Epoch: 0011 loss_train: 1.8292 acc_train: 0.2357 loss_val: 1.8232 acc_val: 0.4400 time: 0.0051s\n",
            "Epoch: 0012 loss_train: 1.8134 acc_train: 0.2929 loss_val: 1.8145 acc_val: 0.3800 time: 0.0049s\n",
            "Epoch: 0013 loss_train: 1.8061 acc_train: 0.3000 loss_val: 1.8062 acc_val: 0.3533 time: 0.0052s\n",
            "Epoch: 0014 loss_train: 1.7890 acc_train: 0.3214 loss_val: 1.7983 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0015 loss_train: 1.7845 acc_train: 0.3143 loss_val: 1.7906 acc_val: 0.3500 time: 0.0054s\n",
            "Epoch: 0016 loss_train: 1.7669 acc_train: 0.3214 loss_val: 1.7833 acc_val: 0.3500 time: 0.0053s\n",
            "Epoch: 0017 loss_train: 1.7643 acc_train: 0.3071 loss_val: 1.7764 acc_val: 0.3500 time: 0.0053s\n",
            "Epoch: 0018 loss_train: 1.7524 acc_train: 0.3143 loss_val: 1.7696 acc_val: 0.3500 time: 0.0064s\n",
            "Epoch: 0019 loss_train: 1.7594 acc_train: 0.2857 loss_val: 1.7629 acc_val: 0.3500 time: 0.0055s\n",
            "Epoch: 0020 loss_train: 1.7569 acc_train: 0.3000 loss_val: 1.7567 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0021 loss_train: 1.7760 acc_train: 0.2857 loss_val: 1.7508 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0022 loss_train: 1.7517 acc_train: 0.2929 loss_val: 1.7451 acc_val: 0.3500 time: 0.0054s\n",
            "Epoch: 0023 loss_train: 1.7423 acc_train: 0.2857 loss_val: 1.7395 acc_val: 0.3500 time: 0.0063s\n",
            "Epoch: 0024 loss_train: 1.7267 acc_train: 0.2929 loss_val: 1.7340 acc_val: 0.3500 time: 0.0050s\n",
            "Epoch: 0025 loss_train: 1.7380 acc_train: 0.3000 loss_val: 1.7286 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0026 loss_train: 1.7431 acc_train: 0.2929 loss_val: 1.7233 acc_val: 0.3500 time: 0.0054s\n",
            "Epoch: 0027 loss_train: 1.7215 acc_train: 0.2929 loss_val: 1.7179 acc_val: 0.3500 time: 0.0063s\n",
            "Epoch: 0028 loss_train: 1.7144 acc_train: 0.2929 loss_val: 1.7126 acc_val: 0.3500 time: 0.0091s\n",
            "Epoch: 0029 loss_train: 1.6957 acc_train: 0.2929 loss_val: 1.7073 acc_val: 0.3500 time: 0.0053s\n",
            "Epoch: 0030 loss_train: 1.6929 acc_train: 0.2929 loss_val: 1.7019 acc_val: 0.3500 time: 0.0055s\n",
            "Epoch: 0031 loss_train: 1.7026 acc_train: 0.2929 loss_val: 1.6966 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0032 loss_train: 1.6601 acc_train: 0.2929 loss_val: 1.6914 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0033 loss_train: 1.6705 acc_train: 0.2857 loss_val: 1.6862 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0034 loss_train: 1.6826 acc_train: 0.2929 loss_val: 1.6809 acc_val: 0.3500 time: 0.0054s\n",
            "Epoch: 0035 loss_train: 1.6496 acc_train: 0.2929 loss_val: 1.6754 acc_val: 0.3500 time: 0.0056s\n",
            "Epoch: 0036 loss_train: 1.6670 acc_train: 0.3000 loss_val: 1.6698 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0037 loss_train: 1.6419 acc_train: 0.3000 loss_val: 1.6638 acc_val: 0.3500 time: 0.0052s\n",
            "Epoch: 0038 loss_train: 1.6544 acc_train: 0.3286 loss_val: 1.6574 acc_val: 0.3500 time: 0.0051s\n",
            "Epoch: 0039 loss_train: 1.6369 acc_train: 0.3071 loss_val: 1.6507 acc_val: 0.3500 time: 0.0054s\n",
            "Epoch: 0040 loss_train: 1.5877 acc_train: 0.3500 loss_val: 1.6435 acc_val: 0.3500 time: 0.0061s\n",
            "Epoch: 0041 loss_train: 1.6160 acc_train: 0.3429 loss_val: 1.6360 acc_val: 0.3500 time: 0.0053s\n",
            "Epoch: 0042 loss_train: 1.6076 acc_train: 0.3000 loss_val: 1.6280 acc_val: 0.3500 time: 0.0053s\n",
            "Epoch: 0043 loss_train: 1.5965 acc_train: 0.3429 loss_val: 1.6196 acc_val: 0.3533 time: 0.0055s\n",
            "Epoch: 0044 loss_train: 1.5578 acc_train: 0.3571 loss_val: 1.6108 acc_val: 0.3633 time: 0.0053s\n",
            "Epoch: 0045 loss_train: 1.5374 acc_train: 0.3643 loss_val: 1.6015 acc_val: 0.3633 time: 0.0054s\n",
            "Epoch: 0046 loss_train: 1.5271 acc_train: 0.3643 loss_val: 1.5919 acc_val: 0.3667 time: 0.0053s\n",
            "Epoch: 0047 loss_train: 1.5160 acc_train: 0.4214 loss_val: 1.5819 acc_val: 0.3667 time: 0.0052s\n",
            "Epoch: 0048 loss_train: 1.5185 acc_train: 0.3786 loss_val: 1.5720 acc_val: 0.3667 time: 0.0055s\n",
            "Epoch: 0049 loss_train: 1.5043 acc_train: 0.3857 loss_val: 1.5617 acc_val: 0.3700 time: 0.0052s\n",
            "Epoch: 0050 loss_train: 1.4955 acc_train: 0.4143 loss_val: 1.5512 acc_val: 0.3767 time: 0.0057s\n",
            "Epoch: 0051 loss_train: 1.4455 acc_train: 0.4643 loss_val: 1.5403 acc_val: 0.3767 time: 0.0053s\n",
            "Epoch: 0052 loss_train: 1.4714 acc_train: 0.4286 loss_val: 1.5294 acc_val: 0.3800 time: 0.0052s\n",
            "Epoch: 0053 loss_train: 1.4702 acc_train: 0.4143 loss_val: 1.5184 acc_val: 0.3867 time: 0.0060s\n",
            "Epoch: 0054 loss_train: 1.4168 acc_train: 0.4714 loss_val: 1.5072 acc_val: 0.4000 time: 0.0106s\n",
            "Epoch: 0055 loss_train: 1.4374 acc_train: 0.4357 loss_val: 1.4958 acc_val: 0.4100 time: 0.0052s\n",
            "Epoch: 0056 loss_train: 1.4190 acc_train: 0.4643 loss_val: 1.4843 acc_val: 0.4300 time: 0.0052s\n",
            "Epoch: 0057 loss_train: 1.4188 acc_train: 0.4714 loss_val: 1.4730 acc_val: 0.4400 time: 0.0051s\n",
            "Epoch: 0058 loss_train: 1.3708 acc_train: 0.5143 loss_val: 1.4616 acc_val: 0.4533 time: 0.0052s\n",
            "Epoch: 0059 loss_train: 1.3560 acc_train: 0.4857 loss_val: 1.4499 acc_val: 0.4667 time: 0.0050s\n",
            "Epoch: 0060 loss_train: 1.3413 acc_train: 0.5786 loss_val: 1.4381 acc_val: 0.4833 time: 0.0051s\n",
            "Epoch: 0061 loss_train: 1.3433 acc_train: 0.5214 loss_val: 1.4263 acc_val: 0.5200 time: 0.0049s\n",
            "Epoch: 0062 loss_train: 1.2971 acc_train: 0.6071 loss_val: 1.4141 acc_val: 0.5467 time: 0.0052s\n",
            "Epoch: 0063 loss_train: 1.3013 acc_train: 0.5571 loss_val: 1.4017 acc_val: 0.5733 time: 0.0051s\n",
            "Epoch: 0064 loss_train: 1.3076 acc_train: 0.6500 loss_val: 1.3890 acc_val: 0.5900 time: 0.0054s\n",
            "Epoch: 0065 loss_train: 1.2642 acc_train: 0.6500 loss_val: 1.3761 acc_val: 0.6100 time: 0.0073s\n",
            "Epoch: 0066 loss_train: 1.2525 acc_train: 0.6357 loss_val: 1.3631 acc_val: 0.6300 time: 0.0075s\n",
            "Epoch: 0067 loss_train: 1.2135 acc_train: 0.6500 loss_val: 1.3504 acc_val: 0.6400 time: 0.0053s\n",
            "Epoch: 0068 loss_train: 1.2264 acc_train: 0.6429 loss_val: 1.3374 acc_val: 0.6400 time: 0.0069s\n",
            "Epoch: 0069 loss_train: 1.2132 acc_train: 0.6786 loss_val: 1.3241 acc_val: 0.6467 time: 0.0058s\n",
            "Epoch: 0070 loss_train: 1.2092 acc_train: 0.6643 loss_val: 1.3108 acc_val: 0.6467 time: 0.0053s\n",
            "Epoch: 0071 loss_train: 1.1725 acc_train: 0.6929 loss_val: 1.2978 acc_val: 0.6600 time: 0.0052s\n",
            "Epoch: 0072 loss_train: 1.1707 acc_train: 0.7000 loss_val: 1.2852 acc_val: 0.6700 time: 0.0054s\n",
            "Epoch: 0073 loss_train: 1.1375 acc_train: 0.7071 loss_val: 1.2728 acc_val: 0.6800 time: 0.0053s\n",
            "Epoch: 0074 loss_train: 1.1043 acc_train: 0.7714 loss_val: 1.2609 acc_val: 0.6933 time: 0.0055s\n",
            "Epoch: 0075 loss_train: 1.1139 acc_train: 0.7429 loss_val: 1.2490 acc_val: 0.7100 time: 0.0106s\n",
            "Epoch: 0076 loss_train: 1.1198 acc_train: 0.7286 loss_val: 1.2371 acc_val: 0.7267 time: 0.0099s\n",
            "Epoch: 0077 loss_train: 1.0755 acc_train: 0.7214 loss_val: 1.2256 acc_val: 0.7400 time: 0.0054s\n",
            "Epoch: 0078 loss_train: 1.0384 acc_train: 0.7571 loss_val: 1.2138 acc_val: 0.7500 time: 0.0065s\n",
            "Epoch: 0079 loss_train: 1.0726 acc_train: 0.7429 loss_val: 1.2020 acc_val: 0.7533 time: 0.0095s\n",
            "Epoch: 0080 loss_train: 1.0338 acc_train: 0.7571 loss_val: 1.1905 acc_val: 0.7600 time: 0.0054s\n",
            "Epoch: 0081 loss_train: 1.0338 acc_train: 0.7643 loss_val: 1.1782 acc_val: 0.7600 time: 0.0050s\n",
            "Epoch: 0082 loss_train: 1.0037 acc_train: 0.7929 loss_val: 1.1658 acc_val: 0.7600 time: 0.0053s\n",
            "Epoch: 0083 loss_train: 0.9612 acc_train: 0.8071 loss_val: 1.1541 acc_val: 0.7567 time: 0.0051s\n",
            "Epoch: 0084 loss_train: 1.0305 acc_train: 0.7643 loss_val: 1.1426 acc_val: 0.7567 time: 0.0053s\n",
            "Epoch: 0085 loss_train: 0.9479 acc_train: 0.8143 loss_val: 1.1317 acc_val: 0.7600 time: 0.0056s\n",
            "Epoch: 0086 loss_train: 0.9579 acc_train: 0.8214 loss_val: 1.1214 acc_val: 0.7733 time: 0.0053s\n",
            "Epoch: 0087 loss_train: 0.9472 acc_train: 0.8000 loss_val: 1.1111 acc_val: 0.7767 time: 0.0051s\n",
            "Epoch: 0088 loss_train: 0.9621 acc_train: 0.7714 loss_val: 1.1021 acc_val: 0.7767 time: 0.0052s\n",
            "Epoch: 0089 loss_train: 0.9310 acc_train: 0.7786 loss_val: 1.0930 acc_val: 0.7800 time: 0.0050s\n",
            "Epoch: 0090 loss_train: 0.9181 acc_train: 0.8214 loss_val: 1.0826 acc_val: 0.7867 time: 0.0052s\n",
            "Epoch: 0091 loss_train: 0.9418 acc_train: 0.7929 loss_val: 1.0723 acc_val: 0.7867 time: 0.0053s\n",
            "Epoch: 0092 loss_train: 0.9083 acc_train: 0.7786 loss_val: 1.0615 acc_val: 0.7900 time: 0.0079s\n",
            "Epoch: 0093 loss_train: 0.8563 acc_train: 0.8643 loss_val: 1.0513 acc_val: 0.7900 time: 0.0051s\n",
            "Epoch: 0094 loss_train: 0.8776 acc_train: 0.8286 loss_val: 1.0417 acc_val: 0.7933 time: 0.0055s\n",
            "Epoch: 0095 loss_train: 0.8890 acc_train: 0.8500 loss_val: 1.0330 acc_val: 0.8000 time: 0.0050s\n",
            "Epoch: 0096 loss_train: 0.8493 acc_train: 0.8357 loss_val: 1.0254 acc_val: 0.8033 time: 0.0052s\n",
            "Epoch: 0097 loss_train: 0.8533 acc_train: 0.8286 loss_val: 1.0181 acc_val: 0.8067 time: 0.0052s\n",
            "Epoch: 0098 loss_train: 0.8434 acc_train: 0.8214 loss_val: 1.0114 acc_val: 0.8067 time: 0.0065s\n",
            "Epoch: 0099 loss_train: 0.8389 acc_train: 0.8214 loss_val: 1.0047 acc_val: 0.8000 time: 0.0050s\n",
            "Epoch: 0100 loss_train: 0.8044 acc_train: 0.8643 loss_val: 0.9981 acc_val: 0.8000 time: 0.0052s\n",
            "Epoch: 0101 loss_train: 0.8042 acc_train: 0.8500 loss_val: 0.9910 acc_val: 0.8000 time: 0.0053s\n",
            "Epoch: 0102 loss_train: 0.8057 acc_train: 0.8143 loss_val: 0.9844 acc_val: 0.8033 time: 0.0050s\n",
            "Epoch: 0103 loss_train: 0.7865 acc_train: 0.8500 loss_val: 0.9785 acc_val: 0.7967 time: 0.0050s\n",
            "Epoch: 0104 loss_train: 0.8064 acc_train: 0.8500 loss_val: 0.9712 acc_val: 0.7967 time: 0.0086s\n",
            "Epoch: 0105 loss_train: 0.7731 acc_train: 0.8500 loss_val: 0.9625 acc_val: 0.8033 time: 0.0052s\n",
            "Epoch: 0106 loss_train: 0.7616 acc_train: 0.8429 loss_val: 0.9543 acc_val: 0.8000 time: 0.0052s\n",
            "Epoch: 0107 loss_train: 0.7904 acc_train: 0.8286 loss_val: 0.9472 acc_val: 0.8000 time: 0.0049s\n",
            "Epoch: 0108 loss_train: 0.7078 acc_train: 0.8714 loss_val: 0.9410 acc_val: 0.7933 time: 0.0049s\n",
            "Epoch: 0109 loss_train: 0.7420 acc_train: 0.8571 loss_val: 0.9352 acc_val: 0.7967 time: 0.0049s\n",
            "Epoch: 0110 loss_train: 0.7333 acc_train: 0.8214 loss_val: 0.9290 acc_val: 0.7967 time: 0.0051s\n",
            "Epoch: 0111 loss_train: 0.7465 acc_train: 0.8429 loss_val: 0.9233 acc_val: 0.8000 time: 0.0050s\n",
            "Epoch: 0112 loss_train: 0.7343 acc_train: 0.8214 loss_val: 0.9174 acc_val: 0.8000 time: 0.0051s\n",
            "Epoch: 0113 loss_train: 0.7201 acc_train: 0.8929 loss_val: 0.9124 acc_val: 0.8067 time: 0.0050s\n",
            "Epoch: 0114 loss_train: 0.7455 acc_train: 0.8500 loss_val: 0.9081 acc_val: 0.8100 time: 0.0051s\n",
            "Epoch: 0115 loss_train: 0.7359 acc_train: 0.8643 loss_val: 0.9037 acc_val: 0.8100 time: 0.0052s\n",
            "Epoch: 0116 loss_train: 0.6912 acc_train: 0.8429 loss_val: 0.8989 acc_val: 0.8067 time: 0.0053s\n",
            "Epoch: 0117 loss_train: 0.7246 acc_train: 0.8571 loss_val: 0.8935 acc_val: 0.8033 time: 0.0049s\n",
            "Epoch: 0118 loss_train: 0.7005 acc_train: 0.8286 loss_val: 0.8881 acc_val: 0.8000 time: 0.0050s\n",
            "Epoch: 0119 loss_train: 0.6734 acc_train: 0.8643 loss_val: 0.8822 acc_val: 0.7967 time: 0.0049s\n",
            "Epoch: 0120 loss_train: 0.6968 acc_train: 0.8714 loss_val: 0.8774 acc_val: 0.7967 time: 0.0049s\n",
            "Epoch: 0121 loss_train: 0.6972 acc_train: 0.8643 loss_val: 0.8732 acc_val: 0.7967 time: 0.0050s\n",
            "Epoch: 0122 loss_train: 0.6540 acc_train: 0.8714 loss_val: 0.8686 acc_val: 0.7967 time: 0.0052s\n",
            "Epoch: 0123 loss_train: 0.6422 acc_train: 0.8857 loss_val: 0.8637 acc_val: 0.7967 time: 0.0051s\n",
            "Epoch: 0124 loss_train: 0.6932 acc_train: 0.8571 loss_val: 0.8600 acc_val: 0.7967 time: 0.0052s\n",
            "Epoch: 0125 loss_train: 0.6551 acc_train: 0.8643 loss_val: 0.8567 acc_val: 0.7967 time: 0.0052s\n",
            "Epoch: 0126 loss_train: 0.7098 acc_train: 0.8571 loss_val: 0.8529 acc_val: 0.8000 time: 0.0051s\n",
            "Epoch: 0127 loss_train: 0.6124 acc_train: 0.9071 loss_val: 0.8487 acc_val: 0.8033 time: 0.0052s\n",
            "Epoch: 0128 loss_train: 0.6197 acc_train: 0.8786 loss_val: 0.8447 acc_val: 0.8067 time: 0.0050s\n",
            "Epoch: 0129 loss_train: 0.6345 acc_train: 0.8857 loss_val: 0.8412 acc_val: 0.8033 time: 0.0053s\n",
            "Epoch: 0130 loss_train: 0.6515 acc_train: 0.8643 loss_val: 0.8376 acc_val: 0.8033 time: 0.0051s\n",
            "Epoch: 0131 loss_train: 0.6548 acc_train: 0.8643 loss_val: 0.8344 acc_val: 0.8033 time: 0.0091s\n",
            "Epoch: 0132 loss_train: 0.6520 acc_train: 0.9000 loss_val: 0.8312 acc_val: 0.8067 time: 0.0052s\n",
            "Epoch: 0133 loss_train: 0.6470 acc_train: 0.8571 loss_val: 0.8281 acc_val: 0.8100 time: 0.0053s\n",
            "Epoch: 0134 loss_train: 0.6202 acc_train: 0.8929 loss_val: 0.8248 acc_val: 0.8133 time: 0.0052s\n",
            "Epoch: 0135 loss_train: 0.6179 acc_train: 0.8786 loss_val: 0.8220 acc_val: 0.8100 time: 0.0052s\n",
            "Epoch: 0136 loss_train: 0.6261 acc_train: 0.8786 loss_val: 0.8190 acc_val: 0.8100 time: 0.0052s\n",
            "Epoch: 0137 loss_train: 0.5942 acc_train: 0.8714 loss_val: 0.8161 acc_val: 0.8100 time: 0.0052s\n",
            "Epoch: 0138 loss_train: 0.5963 acc_train: 0.9071 loss_val: 0.8126 acc_val: 0.8067 time: 0.0050s\n",
            "Epoch: 0139 loss_train: 0.5580 acc_train: 0.9071 loss_val: 0.8095 acc_val: 0.8067 time: 0.0051s\n",
            "Epoch: 0140 loss_train: 0.5933 acc_train: 0.8929 loss_val: 0.8062 acc_val: 0.8033 time: 0.0051s\n",
            "Epoch: 0141 loss_train: 0.6265 acc_train: 0.8643 loss_val: 0.8038 acc_val: 0.8033 time: 0.0098s\n",
            "Epoch: 0142 loss_train: 0.5940 acc_train: 0.8929 loss_val: 0.8015 acc_val: 0.8000 time: 0.0048s\n",
            "Epoch: 0143 loss_train: 0.5760 acc_train: 0.9214 loss_val: 0.7994 acc_val: 0.8067 time: 0.0049s\n",
            "Epoch: 0144 loss_train: 0.6114 acc_train: 0.8500 loss_val: 0.7975 acc_val: 0.8067 time: 0.0049s\n",
            "Epoch: 0145 loss_train: 0.5972 acc_train: 0.9000 loss_val: 0.7945 acc_val: 0.8033 time: 0.0051s\n",
            "Epoch: 0146 loss_train: 0.5483 acc_train: 0.9286 loss_val: 0.7917 acc_val: 0.8067 time: 0.0051s\n",
            "Epoch: 0147 loss_train: 0.5459 acc_train: 0.8857 loss_val: 0.7894 acc_val: 0.8133 time: 0.0052s\n",
            "Epoch: 0148 loss_train: 0.5877 acc_train: 0.9071 loss_val: 0.7867 acc_val: 0.8200 time: 0.0051s\n",
            "Epoch: 0149 loss_train: 0.5656 acc_train: 0.8929 loss_val: 0.7847 acc_val: 0.8200 time: 0.0050s\n",
            "Epoch: 0150 loss_train: 0.5762 acc_train: 0.8643 loss_val: 0.7826 acc_val: 0.8200 time: 0.0052s\n",
            "Epoch: 0151 loss_train: 0.5811 acc_train: 0.9429 loss_val: 0.7802 acc_val: 0.8200 time: 0.0052s\n",
            "Epoch: 0152 loss_train: 0.5494 acc_train: 0.8929 loss_val: 0.7781 acc_val: 0.8200 time: 0.0051s\n",
            "Epoch: 0153 loss_train: 0.5332 acc_train: 0.8929 loss_val: 0.7757 acc_val: 0.8167 time: 0.0051s\n",
            "Epoch: 0154 loss_train: 0.5374 acc_train: 0.9214 loss_val: 0.7723 acc_val: 0.8167 time: 0.0051s\n",
            "Epoch: 0155 loss_train: 0.5433 acc_train: 0.8786 loss_val: 0.7695 acc_val: 0.8067 time: 0.0051s\n",
            "Epoch: 0156 loss_train: 0.5333 acc_train: 0.9214 loss_val: 0.7670 acc_val: 0.8033 time: 0.0052s\n",
            "Epoch: 0157 loss_train: 0.5070 acc_train: 0.9143 loss_val: 0.7647 acc_val: 0.8033 time: 0.0071s\n",
            "Epoch: 0158 loss_train: 0.5275 acc_train: 0.8929 loss_val: 0.7629 acc_val: 0.8000 time: 0.0080s\n",
            "Epoch: 0159 loss_train: 0.5589 acc_train: 0.8929 loss_val: 0.7615 acc_val: 0.8000 time: 0.0050s\n",
            "Epoch: 0160 loss_train: 0.5386 acc_train: 0.9071 loss_val: 0.7601 acc_val: 0.8000 time: 0.0051s\n",
            "Epoch: 0161 loss_train: 0.5476 acc_train: 0.9000 loss_val: 0.7595 acc_val: 0.8000 time: 0.0051s\n",
            "Epoch: 0162 loss_train: 0.5161 acc_train: 0.9071 loss_val: 0.7584 acc_val: 0.8000 time: 0.0050s\n",
            "Epoch: 0163 loss_train: 0.5324 acc_train: 0.9429 loss_val: 0.7579 acc_val: 0.8033 time: 0.0055s\n",
            "Epoch: 0164 loss_train: 0.5326 acc_train: 0.9000 loss_val: 0.7562 acc_val: 0.8067 time: 0.0051s\n",
            "Epoch: 0165 loss_train: 0.5239 acc_train: 0.8857 loss_val: 0.7532 acc_val: 0.8067 time: 0.0051s\n",
            "Epoch: 0166 loss_train: 0.4794 acc_train: 0.9143 loss_val: 0.7496 acc_val: 0.8067 time: 0.0051s\n",
            "Epoch: 0167 loss_train: 0.4924 acc_train: 0.9286 loss_val: 0.7463 acc_val: 0.8100 time: 0.0049s\n",
            "Epoch: 0168 loss_train: 0.5163 acc_train: 0.9000 loss_val: 0.7432 acc_val: 0.8100 time: 0.0050s\n",
            "Epoch: 0169 loss_train: 0.4899 acc_train: 0.9071 loss_val: 0.7406 acc_val: 0.8100 time: 0.0060s\n",
            "Epoch: 0170 loss_train: 0.5400 acc_train: 0.8571 loss_val: 0.7384 acc_val: 0.8100 time: 0.0049s\n",
            "Epoch: 0171 loss_train: 0.4957 acc_train: 0.8929 loss_val: 0.7368 acc_val: 0.8067 time: 0.0052s\n",
            "Epoch: 0172 loss_train: 0.5064 acc_train: 0.9357 loss_val: 0.7352 acc_val: 0.8067 time: 0.0050s\n",
            "Epoch: 0173 loss_train: 0.5353 acc_train: 0.9071 loss_val: 0.7336 acc_val: 0.8067 time: 0.0054s\n",
            "Epoch: 0174 loss_train: 0.5033 acc_train: 0.9000 loss_val: 0.7324 acc_val: 0.8067 time: 0.0051s\n",
            "Epoch: 0175 loss_train: 0.4756 acc_train: 0.9143 loss_val: 0.7315 acc_val: 0.8067 time: 0.0069s\n",
            "Epoch: 0176 loss_train: 0.5159 acc_train: 0.8929 loss_val: 0.7314 acc_val: 0.8033 time: 0.0053s\n",
            "Epoch: 0177 loss_train: 0.4964 acc_train: 0.9214 loss_val: 0.7319 acc_val: 0.8067 time: 0.0054s\n",
            "Epoch: 0178 loss_train: 0.4936 acc_train: 0.9429 loss_val: 0.7330 acc_val: 0.8067 time: 0.0052s\n",
            "Epoch: 0179 loss_train: 0.5320 acc_train: 0.8929 loss_val: 0.7338 acc_val: 0.8067 time: 0.0054s\n",
            "Epoch: 0180 loss_train: 0.4773 acc_train: 0.9286 loss_val: 0.7341 acc_val: 0.8100 time: 0.0052s\n",
            "Epoch: 0181 loss_train: 0.5001 acc_train: 0.9214 loss_val: 0.7337 acc_val: 0.8033 time: 0.0053s\n",
            "Epoch: 0182 loss_train: 0.4684 acc_train: 0.9214 loss_val: 0.7328 acc_val: 0.8033 time: 0.0051s\n",
            "Epoch: 0183 loss_train: 0.4816 acc_train: 0.9143 loss_val: 0.7311 acc_val: 0.8033 time: 0.0063s\n",
            "Epoch: 0184 loss_train: 0.4677 acc_train: 0.9429 loss_val: 0.7281 acc_val: 0.8033 time: 0.0097s\n",
            "Epoch: 0185 loss_train: 0.4986 acc_train: 0.9143 loss_val: 0.7244 acc_val: 0.8033 time: 0.0054s\n",
            "Epoch: 0186 loss_train: 0.4449 acc_train: 0.9286 loss_val: 0.7216 acc_val: 0.8000 time: 0.0052s\n",
            "Epoch: 0187 loss_train: 0.5297 acc_train: 0.9143 loss_val: 0.7187 acc_val: 0.8000 time: 0.0054s\n",
            "Epoch: 0188 loss_train: 0.4796 acc_train: 0.9143 loss_val: 0.7162 acc_val: 0.8000 time: 0.0052s\n",
            "Epoch: 0189 loss_train: 0.4669 acc_train: 0.9286 loss_val: 0.7145 acc_val: 0.8067 time: 0.0053s\n",
            "Epoch: 0190 loss_train: 0.4769 acc_train: 0.9143 loss_val: 0.7135 acc_val: 0.8033 time: 0.0053s\n",
            "Epoch: 0191 loss_train: 0.4716 acc_train: 0.9143 loss_val: 0.7132 acc_val: 0.8067 time: 0.0055s\n",
            "Epoch: 0192 loss_train: 0.4409 acc_train: 0.9357 loss_val: 0.7140 acc_val: 0.8067 time: 0.0068s\n",
            "Epoch: 0193 loss_train: 0.4927 acc_train: 0.9214 loss_val: 0.7163 acc_val: 0.8100 time: 0.0052s\n",
            "Epoch: 0194 loss_train: 0.4548 acc_train: 0.9286 loss_val: 0.7177 acc_val: 0.8100 time: 0.0050s\n",
            "Epoch: 0195 loss_train: 0.4770 acc_train: 0.9071 loss_val: 0.7181 acc_val: 0.8100 time: 0.0054s\n",
            "Epoch: 0196 loss_train: 0.4270 acc_train: 0.9571 loss_val: 0.7181 acc_val: 0.8100 time: 0.0053s\n",
            "Epoch: 0197 loss_train: 0.4593 acc_train: 0.9500 loss_val: 0.7166 acc_val: 0.8067 time: 0.0052s\n",
            "Epoch: 0198 loss_train: 0.4690 acc_train: 0.9286 loss_val: 0.7153 acc_val: 0.8067 time: 0.0050s\n",
            "Epoch: 0199 loss_train: 0.4627 acc_train: 0.9286 loss_val: 0.7149 acc_val: 0.8067 time: 0.0051s\n",
            "Epoch: 0200 loss_train: 0.4525 acc_train: 0.9286 loss_val: 0.7135 acc_val: 0.8067 time: 0.0051s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 3.9699s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "test()"
      ],
      "metadata": {
        "id": "wdgqvRpVkFSN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eaefe1c-e7ce-4c09-f7db-c575622af108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set results: loss= 0.7517 accuracy= 0.8360\n"
          ]
        }
      ]
    }
  ]
}